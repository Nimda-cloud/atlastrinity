"""AtlasTrinity Full Stack Development Setup Script
–í–∏–∫–æ–Ω—É—î –∫–æ–º–ø–ª–µ–∫—Å–Ω–µ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞ –ø—ñ—Å–ª—è –∫–ª–æ–Ω—É–≤–∞–Ω–Ω—è:
- –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞ (Python 3.12.12, Bun, Swift)
- –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–∞ —Å–∏–Ω—Ö—Ä–æ–Ω—ñ–∑–∞—Ü—ñ—è –≥–ª–æ–±–∞–ª—å–Ω–∏—Ö –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π (~/.config/atlastrinity)
- –ö–æ–º–ø—ñ–ª—è—Ü—ñ—è –Ω–∞—Ç–∏–≤–Ω–∏—Ö MCP —Å–µ—Ä–≤–µ—Ä—ñ–≤ (Swift)
- –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è Python —Ç–∞ NPM –∑–∞–ª–µ–∂–Ω–æ—Å—Ç–µ–π
- –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è AI –º–æ–¥–µ–ª–µ–π (STT/TTS)
- –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Å–∏—Å—Ç–µ–º–Ω–∏—Ö —Å–µ—Ä–≤—ñ—Å—ñ–≤ (Redis, Vibe CLI)
"""

import argparse
import asyncio
import json
import os
import platform
import re
import select
import shutil
import sqlite3
import subprocess
import sys
import time
from pathlib import Path


# –ö–æ–ª—å–æ—Ä–∏ –¥–ª—è –∫–æ–Ω—Å–æ–ª—ñ
class Colors:
    HEADER = "\033[95m"
    OKBLUE = "\033[94m"
    OKCYAN = "\033[96m"
    OKGREEN = "\033[92m"
    WARNING = "\033[93m"
    FAIL = "\033[91m"
    ENDC = "\033[0m"
    BOLD = "\033[1m"


class InstallationTracker:
    """Tracks the status of each setup phase for the final report."""

    def __init__(self):
        self.results = {
            "DB Restore": ("Pending", Colors.WARNING or "\033[93m"),
            "venv Creation": ("Pending", Colors.WARNING or "\033[93m"),
            "Python Packages": ("Pending", Colors.WARNING or "\033[93m"),
            "NPM Packages": ("Pending", Colors.WARNING or "\033[93m"),
            "Swift Build (macos-use)": ("Pending", Colors.WARNING or "\033[93m"),
            "Model Downloads": ("Pending", Colors.WARNING or "\033[93m"),
            "DB Initialization": ("Pending", Colors.WARNING or "\033[93m"),
            "Knowledge Base Setup": ("Pending", Colors.WARNING or "\033[93m"),
            "MCP Tools Integration": ("Pending", Colors.WARNING or "\033[93m"),
        }

    def update(self, key, status, color=None):
        if color is None:
            color = Colors.OKGREEN
        if key in self.results:
            self.results[key] = (status, color)

    def print_report(self):
        print(f"\n{Colors.OKCYAN}{Colors.BOLD}üéâ –û—á—ñ–∫—É–≤–∞–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:{Colors.ENDC}")
        for key, (status, color) in self.results.items():
            icon = "‚úÖ" if color == Colors.OKGREEN else "‚ö†Ô∏è" if color == Colors.WARNING else "‚ùå"
            print(f"  {icon} {key:<40} {color}{status}{Colors.ENDC}")
        print()


tracker = InstallationTracker()


def print_step(msg: str):
    pass


def print_success(msg: str):
    pass


def print_warning(msg: str):
    pass


def print_error(msg: str):
    pass


def print_info(msg: str):
    pass


# –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∏
REQUIRED_PYTHON = "3.12.12"
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

CONFIG_ROOT = Path.home() / ".config" / "atlastrinity"
VENV_PATH = PROJECT_ROOT / ".venv"

# –ü–∞–ø–∫–∏ –¥–ª—è –∫–æ–Ω—Ñ—ñ–≥—ñ–≤ —Ç–∞ –º–æ–¥–µ–ª–µ–π
DIRS = {
    "config": CONFIG_ROOT,
    "logs": CONFIG_ROOT / "logs",
    "memory": CONFIG_ROOT / "memory",
    "screenshots": CONFIG_ROOT / "screenshots",
    "tts_models": CONFIG_ROOT / "models" / "tts",
    "stt_models": CONFIG_ROOT / "models" / "faster-whisper",
    "mcp": CONFIG_ROOT / "mcp",
    "workspace": CONFIG_ROOT / "workspace",
    "vibe_workspace": CONFIG_ROOT / "vibe_workspace",
    "stanza": CONFIG_ROOT / "models" / "stanza",
    "huggingface": CONFIG_ROOT / "models" / "huggingface",
}


def check_python_version():
    """–ü–µ—Ä–µ–≤—ñ—Ä—è—î –≤–µ—Ä—Å—ñ—é Python"""
    print_step(f"–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –≤–µ—Ä—Å—ñ—ó Python (—Ü—ñ–ª—å: {REQUIRED_PYTHON})...")
    current_version = platform.python_version()

    if current_version == REQUIRED_PYTHON:
        print_success(f"Python {current_version} –∑–Ω–∞–π–¥–µ–Ω–æ")
        return True

    print_warning(f"–ü–æ—Ç–æ—á–Ω–∞ –≤–µ—Ä—Å—ñ—è Python: {current_version}")

    # Proactive installation/fix
    if shutil.which("brew"):
        print_info(f"–°–ø—Ä–æ–±–∞ –æ–Ω–æ–≤–∏—Ç–∏ Python –¥–æ {REQUIRED_PYTHON} —á–µ—Ä–µ–∑ Homebrew...")
        try:
            # brew install python@3.12 installs the latest in that branch
            # If we want EXACTLY 3.12.12, brew might not provide it as a separate formula easily
            # but we can try to update the branch
            subprocess.run(["brew", "upgrade", "python@3.12"], check=False)

            # Re-check after potential upgrade
            # Wait a bit for filesystem
            time.sleep(1)
            # We can't easily change the running process python version,
            # so we inform the user to restart if we detect a change was made or still mismatched.
            print_info(f"–†–µ–∫–æ–º–µ–Ω–¥—É—î–º–æ –ø–µ—Ä–µ–∫–æ–Ω–∞—Ç–∏—Å—è, —â–æ —É –≤–∞—Å –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ {REQUIRED_PYTHON}.")
            print_info(
                "–Ø–∫—â–æ –≤–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç–µ pyenv: pyenv install 3.12.12 && pyenv global 3.12.12"
            )
        except Exception as e:
            print_warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –æ–Ω–æ–≤–∏—Ç–∏ Python: {e}")

    return True  # –î–æ–∑–≤–æ–ª—è—î–º–æ –ø—Ä–æ–¥–æ–≤–∂–∏—Ç–∏, –∞–ª–µ –∑ –ø–æ–ø–µ—Ä–µ–¥–∂–µ–Ω–Ω—è–º


def ensure_directories():
    """–°—Ç–≤–æ—Ä—é—î –Ω–µ–æ–±—Ö—ñ–¥–Ω—ñ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó –≤ ~/.config"""
    print_step("–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –≥–ª–æ–±–∞–ª—å–Ω–∏—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ–π...")
    for name, path in DIRS.items():
        if not path.exists():
            path.mkdir(parents=True, exist_ok=True)
            print_success(f"–°—Ç–≤–æ—Ä–µ–Ω–æ {name}: {path}")
        else:
            print_success(f"–î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è {name} –≤–∂–µ —ñ—Å–Ω—É—î")


def check_system_tools():
    """–ü–µ—Ä–µ–≤—ñ—Ä—è—î –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –±–∞–∑–æ–≤–∏—Ö —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤ —Ç–∞ –≤—Å—Ç–∞–Ω–æ–≤–ª—é—î –≤—ñ–¥—Å—É—Ç–Ω—ñ"""
    print_step("–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –±–∞–∑–æ–≤–∏—Ö —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤ (Brew, Python, Node, Bun)...")

    # Ensure Homebrew is in PATH
    brew_paths = ["/opt/homebrew/bin", "/usr/local/bin"]
    for p in brew_paths:
        if p not in os.environ["PATH"]:
            os.environ["PATH"] = p + os.pathsep + os.environ["PATH"]

    if not shutil.which("brew"):
        print_error("Homebrew –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ! –í—ñ–Ω –∫—Ä–∏—Ç–∏—á–Ω–æ –Ω–µ–æ–±—Ö—ñ–¥–Ω–∏–π.")
        return False

    # 1. Ensure Python 3.12
    if not shutil.which("python3.12"):
        print_info("Python 3.12 –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è —á–µ—Ä–µ–∑ Brew...")
        try:
            subprocess.run(["brew", "install", "python@3.12"], check=True)
            print_success("Python 3.12 –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
        except Exception as e:
            print_error(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –≤—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏ Python 3.12: {e}")
    else:
        print_success("Python 3.12 –∑–Ω–∞–π–¥–µ–Ω–æ")

    # 2. Ensure Node 22
    if not shutil.which("node") or "v22" not in subprocess.getoutput("node --version"):
        print_info("Node 22 –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∞–±–æ –≤–µ—Ä—Å—ñ—è –∑–∞—Å—Ç–∞—Ä—ñ–ª–∞. –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è node@22 —á–µ—Ä–µ–∑ Brew...")
        try:
            subprocess.run(["brew", "install", "node@22"], check=True)
            # Link node@22 if possible
            subprocess.run(["brew", "link", "--overwrite", "node@22"], check=False)

            # Export path for current session
            node_path = "/opt/homebrew/opt/node@22/bin"
            if os.path.exists(node_path):
                os.environ["PATH"] = node_path + os.pathsep + os.environ["PATH"]
            print_success("Node 22 –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
        except Exception as e:
            print_error(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –≤—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏ Node 22: {e}")
    else:
        print_success(f"Node –∑–Ω–∞–π–¥–µ–Ω–æ ({subprocess.getoutput('node --version').strip()})")

    # 3. Check other tools (includes all 13 linters from lint:all + build tools)
    tools = [
        "safety",
        "detect-secrets",
        "gcloud",
        "actionlint",
    ]
    # Python tools that live in .venv/bin
    venv_tools = {
        "ruff",
        "pyrefly",
        "vulture",
        "bandit",
        "xenon",
        "safety",
        "detect-secrets",
        "zizmor",
    }
    missing = []

    for tool in tools:
        path = shutil.which(tool)
        if path:
            print_success(f"{tool} –∑–Ω–∞–π–¥–µ–Ω–æ")
        else:
            # Check venv for python tools (only if venv exists)
            if tool in venv_tools:
                venv_tool = PROJECT_ROOT / ".venv" / "bin" / tool
                if venv_tool.exists():
                    print_success(f"{tool} –∑–Ω–∞–π–¥–µ–Ω–æ —É .venv")
                    continue

            if tool in ["bun", "swift", "npm"] or tool == "vibe":
                print_warning(f"{tool} –ù–ï –∑–Ω–∞–π–¥–µ–Ω–æ")
                missing.append(tool)
            else:
                # Python-specific tools are non-blocking here as they'll be installed later
                print_info(f"{tool} –ø–æ–∫–∏ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ (–±—É–¥–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ —É .venv –ø—ñ–∑–Ω—ñ—à–µ)")

    # Check npx-based tools (biome, eslint, pyright, tsc, lefthook are npm devDependencies)
    npx_tools = ["@biomejs/biome", "eslint", "pyright", "lefthook"]
    for pkg in npx_tools:
        pkg_dir = PROJECT_ROOT / "node_modules" / pkg
        if pkg_dir.exists():
            print_success(f"{pkg} –∑–Ω–∞–π–¥–µ–Ω–æ –≤ node_modules")
        else:
            print_info(f"{pkg} –ø–æ–∫–∏ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ (–±—É–¥–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ —á–µ—Ä–µ–∑ npm install)")

    # Auto-install Vibe if missing
    if "vibe" in missing:
        print_info("Vibe CLI –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è Vibe...")
        try:
            subprocess.run(
                "curl -LsSf https://mistral.ai/vibe/install.sh | bash", shell=True, check=True
            )
            # Add to PATH for current session (Vibe installs to ~/.local/bin)
            vibe_bin = Path.home() / ".local" / "bin"
            os.environ["PATH"] = str(vibe_bin) + os.pathsep + os.environ.get("PATH", "")
            print_success("Vibe CLI –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
            if "vibe" in missing:
                missing.remove("vibe")
        except Exception as e:
            print_error(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –≤—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏ Vibe CLI: {e}")

    # Auto-install Bun if missing
    if "bun" in missing:
        print_info("Bun –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è Bun...")
        try:
            subprocess.run("curl -fsSL https://bun.sh/install | bash", shell=True, check=True)
            # Add to PATH for current session
            bun_bin = Path.home() / ".bun" / "bin"
            os.environ["PATH"] += os.pathsep + str(bun_bin)
            print_success("Bun –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
            if "bun" in missing:
                missing.remove("bun")
        except Exception as e:
            print_error(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –≤—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏ Bun: {e}")

    # Auto-install JS dev tools if missing
    if any(t in missing for t in ["oxlint", "knip"]):
        print_info("–î–µ—è–∫—ñ JS —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ (oxlint/knip) –≤—ñ–¥—Å—É—Ç–Ω—ñ. –°–ø—Ä–æ–±–∞ –≤—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏ —á–µ—Ä–µ–∑ npm...")
        try:
            subprocess.run(["npm", "install", "-g", "oxlint", "knip"], check=False)
            print_success("JS —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
            for t in ["oxlint", "knip"]:
                if t in missing:
                    missing.remove(t)
        except Exception:
            pass

    # Auto-install gcloud if missing
    if "gcloud" in missing:
        print_info("gcloud CLI –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è Google Cloud SDK —á–µ—Ä–µ–∑ Brew...")
        try:
            subprocess.run(["brew", "install", "--cask", "google-cloud-sdk"], check=True)
            # Brew usually symlinks gcloud to common paths, but we ensure it's in PATH
            print_success("Google Cloud SDK (gcloud) –±—É–ª–æ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
            if "gcloud" in missing:
                missing.remove("gcloud")
        except Exception as e:
            print_warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏ Google Cloud SDK: {e}")
            print_info(
                "–ë—É–¥—å –ª–∞—Å–∫–∞, –≤—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å –π–æ–≥–æ –≤—Ä—É—á–Ω—É: https://cloud.google.com/sdk/docs/install"
            )

    if "swift" in missing:
        print_error("Swift –Ω–µ–æ–±—Ö—ñ–¥–Ω–∏–π –¥–ª—è –∫–æ–º–ø—ñ–ª—è—Ü—ñ—ó macos-use —Ç–∞ googlemaps MCP —Å–µ—Ä–≤–µ—Ä—ñ–≤!")
        print_info("–í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å Xcode –∞–±–æ Command Line Tools: xcode-select --install")

    # Auto-install GitHub CLI if missing
    if "gh" in missing:
        print_info("GitHub CLI (gh) –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è —á–µ—Ä–µ–∑ Brew...")
        try:
            subprocess.run(["brew", "install", "gh"], check=True)
            print_success("GitHub CLI –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
            if "gh" in missing:
                missing.remove("gh")
        except Exception as e:
            print_warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –≤—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏ GitHub CLI: {e}")

    # 4. Check for Full Xcode (for XcodeBuildMCP)
    # Search multiple locations: standard, beta, user Desktop, Spotlight
    xcode_candidates = [
        Path("/Applications/Xcode.app"),
        Path("/Applications/Xcode-beta.app"),
        Path.home() / "Desktop" / "Xcode.app",
        Path.home() / "Desktop" / "Xcode-beta.app",
    ]
    # Also search via Spotlight for non-standard locations
    try:
        spotlight = subprocess.run(
            ["mdfind", "kMDItemCFBundleIdentifier == 'com.apple.dt.Xcode'"],
            capture_output=True,
            text=True,
            check=False,
            timeout=5,
        )
        for line in spotlight.stdout.strip().splitlines():
            p = Path(line.strip())
            if p.exists() and p not in xcode_candidates:
                xcode_candidates.append(p)
    except Exception:
        pass

    xcode_app = next((p for p in xcode_candidates if p.exists()), None)

    if shutil.which("xcodebuild"):
        result = subprocess.run(
            ["xcodebuild", "-version"], capture_output=True, text=True, check=False
        )
        if result.returncode == 0:
            print_success("–ü–æ–≤–Ω–∏–π Xcode –∑–Ω–∞–π–¥–µ–Ω–æ")
        elif xcode_app:
            print_warning(f"xcode-select –≤–∫–∞–∑—É—î –Ω–∞ CLT, –∞–ª–µ Xcode –∑–Ω–∞–π–¥–µ–Ω–æ: {xcode_app}")
            xcode_dev_path = xcode_app / "Contents" / "Developer"
            print_info(f"–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–Ω—è: sudo xcode-select -s {xcode_dev_path}")
            try:
                subprocess.run(["sudo", "xcode-select", "-s", str(xcode_dev_path)], check=True)
                print_success(f"–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–æ –Ω–∞ –ø–æ–≤–Ω–∏–π Xcode ({xcode_app.name})")
            except Exception as e:
                print_error(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç–∏—Å—å: {e}")
                print_info(f"–°–ø—Ä–æ–±—É–π—Ç–µ –≤—Ä—É—á–Ω—É: sudo xcode-select -s {xcode_dev_path}")
        else:
            print_warning("–ó–Ω–∞–π–¥–µ–Ω–æ —Ç—ñ–ª—å–∫–∏ Command Line Tools (–ø–æ–≤–Ω–∏–π Xcode –≤—ñ–¥—Å—É—Ç–Ω—ñ–π)")
            print_info("XcodeBuildMCP –±—É–¥–µ –ø—Ä–æ–ø—É—â–µ–Ω–æ –ø—Ä–∏ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—ñ.")
    elif xcode_app:
        xcode_dev_path = xcode_app / "Contents" / "Developer"
        print_warning(f"Xcode –∑–Ω–∞–π–¥–µ–Ω–æ ({xcode_app}), –∞–ª–µ –Ω–µ –∞–∫—Ç–∏–≤–æ–≤–∞–Ω–æ!")
        print_info(f"–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–Ω—è: sudo xcode-select -s {xcode_dev_path}")
        try:
            subprocess.run(["sudo", "xcode-select", "-s", str(xcode_dev_path)], check=True)
            print_success(f"–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–æ –Ω–∞ –ø–æ–≤–Ω–∏–π Xcode ({xcode_app.name})")
        except Exception as e:
            print_error(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç–∏—Å—å: {e}")
    else:
        print_warning("Xcode –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
        print_info("XcodeBuildMCP –±—É–¥–µ –ø—Ä–æ–ø—É—â–µ–Ω–æ –ø—Ä–∏ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—ñ.")

    return "brew" not in missing  # Brew —î –æ–±–æ–≤'—è–∑–∫–æ–≤–∏–º


def ensure_database():
    """Initialize SQLite database in global config folder"""
    print_step("–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –æ—Å–Ω–æ–≤–Ω–æ—ó –±–∞–∑–∏ –¥–∞–Ω–∏—Ö (SQLite)...")
    db_path = CONFIG_ROOT / "atlastrinity.db"
    backup_path = PROJECT_ROOT / "backups" / "databases" / "atlastrinity.db"

    # 1. Restore from backup if exists and local is missing
    if not db_path.exists() and backup_path.exists():
        print_info("–í—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –æ—Å–Ω–æ–≤–Ω–æ—ó –±–∞–∑–∏ –∑ –±–µ–∫–∞–ø—É —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä—ñ—é...")
        try:
            shutil.copy2(backup_path, db_path)
            print_success("–ë–∞–∑—É –¥–∞–Ω–∏—Ö –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–æ")
        except Exception as e:
            print_warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –≤—ñ–¥–Ω–æ–≤–∏—Ç–∏ –±–∞–∑—É: {e}")

    # 2. Check if database file exists
    try:
        if db_path.exists():
            print_success(f"SQLite –±–∞–∑–∞ –¥–∞–Ω–∏—Ö –≤–∂–µ —ñ—Å–Ω—É—î: {db_path}")
        else:
            print_info(f"–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–æ–≤–æ—ó SQLite –±–∞–∑–∏: {db_path}...")

        # 3. Initialize tables via SQLAlchemy
        print_info("–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ç–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Ç–∞–±–ª–∏—Ü—å (SQLAlchemy)...")
        venv_python = str(VENV_PATH / "bin" / "python")
        init_cmd = [venv_python, str(PROJECT_ROOT / "src" / "maintenance" / "init_db.py")]
        subprocess.run(init_cmd, cwd=PROJECT_ROOT, check=True)
        print_success("–°—Ö–µ–º—É –±–∞–∑–∏ –¥–∞–Ω–∏—Ö –ø–µ—Ä–µ–≤—ñ—Ä–µ–Ω–æ/—ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ")

    except Exception as e:
        print_warning(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—ñ –ë–î: {e}")

    # 4. Ensure legacy trinity.db exists for backup consistency
    trinity_db_path = CONFIG_ROOT / "data" / "trinity.db"
    trinity_db_path.parent.mkdir(parents=True, exist_ok=True)
    if not trinity_db_path.exists():
        print_info("–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ—Ä–æ–∂–Ω—å–æ—ó legacy –±–∞–∑–∏ trinity.db...")
        try:
            with sqlite3.connect(trinity_db_path) as conn:
                conn.execute("PRAGMA journal_mode=WAL;")
                conn.commit()
            print_success("Legacy trinity.db —Å—Ç–≤–æ—Ä–µ–Ω–æ")
        except Exception as e:
            print_warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è —Å—Ç–≤–æ—Ä–∏—Ç–∏ legacy trinity.db: {e}")


def prepare_monitoring_db():
    """Initialize Monitoring SQLite database"""
    print_step("–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –±–∞–∑–∏ –¥–∞–Ω–∏—Ö –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É (SQLite)...")
    monitor_db_path = CONFIG_ROOT / "data" / "monitoring.db"
    monitor_db_path.parent.mkdir(parents=True, exist_ok=True)

    if monitor_db_path.exists():
        print_success(f"Monitoring DB –≤–∂–µ —ñ—Å–Ω—É—î: {monitor_db_path}")
    else:
        print_info(f"Monitoring DB –±—É–¥–µ —Å—Ç–≤–æ—Ä–µ–Ω–æ –ø—Ä–∏ –ø–µ—Ä—à–æ–º—É –∑–∞–ø—É—Å–∫—É: {monitor_db_path}")

    # Backup restore logic could go here if we persist monitoring data across resets
    backup_path = PROJECT_ROOT / "backups" / "databases" / "monitoring.db"
    if not monitor_db_path.exists() and backup_path.exists():
        try:
            shutil.copy2(backup_path, monitor_db_path)
            print_success("Monitoring DB –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–æ –∑ –±–µ–∫–∞–ø—É")
        except Exception as e:
            print_warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –≤—ñ–¥–Ω–æ–≤–∏—Ç–∏ Monitoring DB: {e}")

    # Ensure tables exist (Schema Check)
    try:
        with sqlite3.connect(monitor_db_path) as conn:
            # Check for healing_events
            conn.execute("""
                CREATE TABLE IF NOT EXISTS healing_events (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT,
                    task_id TEXT,
                    event_type TEXT, -- 'auto_healing', 'constraint_violation'
                    step_id TEXT,
                    priority INTEGER,
                    status TEXT,
                    details JSON
                )
            """)

            # Also ensure basic logs tables exist (sync with monitoring.py)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS logs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT,
                    level TEXT,
                    service TEXT,
                    message TEXT,
                    data JSON
                )
            """)
            conn.commit()
        print_success("–°—Ö–µ–º–∞ Monitoring DB (–≤–∫–ª—é—á–∞—é—á–∏ healing_events) –ø–µ—Ä–µ–≤—ñ—Ä–µ–Ω–∞")
    except Exception as e:
        print_warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –æ–Ω–æ–≤–∏—Ç–∏ —Å—Ö–µ–º—É Monitoring DB: {e}")


def verify_golden_fund():
    """Verify Golden Fund database and restore from backup if needed."""
    print_step("–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ Golden Fund (Backup & Restore)...")

    # Paths
    config_db_dir = CONFIG_ROOT / "data" / "golden_fund"
    config_db_path = config_db_dir / "golden.db"

    backup_repo_dir = PROJECT_ROOT / "backups" / "databases" / "golden_fund"

    # 1. Restore the entire Golden Fund directory (DB + Vectors + Cache)
    if not config_db_dir.exists() or not list(config_db_dir.glob("*")):
        if backup_repo_dir.exists():
            print_info("–í—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è Golden Fund (–±–∞–∑–∞ + –≤–µ–∫—Ç–æ—Ä–∏) –∑ –±–µ–∫–∞–ø—É —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä—ñ—é...")
            try:
                if config_db_dir.exists():
                    shutil.rmtree(config_db_dir)
                shutil.copytree(backup_repo_dir, config_db_dir)
                print_success("Golden Fund –ø–æ–≤–Ω—ñ—Å—Ç—é –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–æ –∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä—ñ—é")
            except Exception as e:
                print_error(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –≤—ñ–¥–Ω–æ–≤–∏—Ç–∏ Golden Fund: {e}")
        else:
            print_info("–ë–µ–∫–∞–ø Golden Fund –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. –ë—É–¥–µ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–æ –Ω–æ–≤—É –±–∞–∑—É.")

    # Ensure config directory exists (if not restored)
    config_db_dir.mkdir(parents=True, exist_ok=True)

    # Initialize Golden Fund database if it doesn't exist
    if not config_db_path.exists():
        print_info("–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–æ–≤–æ—ó –±–∞–∑–∏ –¥–∞–Ω–∏—Ö Golden Fund...")
        try:
            with sqlite3.connect(config_db_path) as conn:
                # Enable WAL mode for better concurrency
                conn.execute("PRAGMA journal_mode=WAL;")
                # Create a metadata table to track datasets
                conn.execute(
                    """
                    CREATE TABLE IF NOT EXISTS datasets_metadata (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        dataset_name TEXT UNIQUE,
                        table_name TEXT,
                        source_url TEXT,
                        ingested_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        row_count INTEGER
                    )
                    """
                )
                conn.commit()
            print_success("–ë–∞–∑—É –¥–∞–Ω–∏—Ö Golden Fund —É—Å–ø—ñ—à–Ω–æ —Å—Ç–≤–æ—Ä–µ–Ω–æ")
        except Exception as e:
            print_error(f"–ù–µ –≤–¥–∞–ª–æ—Å—è —Å—Ç–≤–æ—Ä–∏—Ç–∏ Golden Fund: {e}")
            return

    # Initialize Golden Fund ChromaDB if it doesn't exist
    golden_fund_chroma_dir = config_db_dir / "chroma_db"
    if not golden_fund_chroma_dir.exists():
        print_info("–°—Ç–≤–æ—Ä–µ–Ω–Ω—è Golden Fund ChromaDB...")
        try:
            golden_fund_chroma_dir.mkdir(parents=True, exist_ok=True)
            print_success("Golden Fund ChromaDB —Å—Ç–≤–æ—Ä–µ–Ω–æ")
        except Exception as e:
            print_warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è —Å—Ç–≤–æ—Ä–∏—Ç–∏ Golden Fund ChromaDB: {e}")

    # 2. Support Memory Chroma restore
    memory_chroma_dir = CONFIG_ROOT / "memory" / "chroma"
    backup_memory_dir = PROJECT_ROOT / "backups" / "databases" / "memory" / "chroma"

    if not memory_chroma_dir.exists() and backup_memory_dir.exists():
        print_info("–í—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è Memory Chroma (—Å–µ–º–∞–Ω—Ç–∏–∫–∞/–≥—Ä–∞—Ñ–∏) –∑ –±–µ–∫–∞–ø—É...")
        try:
            memory_chroma_dir.parent.mkdir(parents=True, exist_ok=True)
            shutil.copytree(backup_memory_dir, memory_chroma_dir)
            print_success("Memory Chroma –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–æ")
        except Exception as e:
            print_warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –≤—ñ–¥–Ω–æ–≤–∏—Ç–∏ Memory Chroma: {e}")

    # 2. Check Tables (Verify Integrity)

    try:
        # We can't use the SQLStorage class directly easily here without import setup, so raw check
        with sqlite3.connect(config_db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
            tables = [row[0] for row in cursor.fetchall()]

            required_tables = ["datasets_metadata"]
            missing = [t for t in required_tables if t not in tables]

            if missing:
                print_warning(f"–í—ñ–¥—Å—É—Ç–Ω—ñ —Ç–∞–±–ª–∏—Ü—ñ –≤ Golden Fund: {missing}")
                # Ideally we would trigger re-init here, but the server handles that on startup
                print_info("–°–µ—Ä–≤–µ—Ä Golden Fund —Å—Ç–≤–æ—Ä–∏—Ç—å –Ω–µ–æ–±—Ö—ñ–¥–Ω—ñ —Ç–∞–±–ª–∏—Ü—ñ –ø—Ä–∏ –∑–∞–ø—É—Å–∫—É.")
            else:
                print_success(f"Golden Fund –ø–µ—Ä–µ–≤—ñ—Ä–µ–Ω–æ: {len(tables)} —Ç–∞–±–ª–∏—Ü—å –∑–Ω–∞–π–¥–µ–Ω–æ")
                tracker.update("Knowledge Base Setup", "Verified")

    except Exception as e:
        print_warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –ø–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É Golden Fund: {e}")

    # 3. Ensure Search Index DB exists
    search_index_path = CONFIG_ROOT / "data" / "search" / "golden_fund_index.db"
    search_index_path.parent.mkdir(parents=True, exist_ok=True)

    if not search_index_path.exists():
        print_info("–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –±–∞–∑–∏ —ñ–Ω–¥–µ–∫—Å—É –ø–æ—à—É–∫—É (FTS5)...")
        try:
            with sqlite3.connect(search_index_path) as conn:
                conn.execute("PRAGMA journal_mode=WAL;")
                # FTS table will be created by the ingest tool, but we ensure the file exists
                conn.commit()
            print_success("Search Index DB —Å—Ç–≤–æ—Ä–µ–Ω–æ")
        except Exception as e:
            print_warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è —Å—Ç–≤–æ—Ä–∏—Ç–∏ Search Index DB: {e}")


def _brew_formula_installed(formula: str) -> bool:
    rc = subprocess.run(["brew", "list", "--formula", formula], check=False, capture_output=True)
    return rc.returncode == 0


def _brew_cask_installed(cask: str, app_name: str) -> bool:
    # 1) check brew metadata
    rc = subprocess.run(["brew", "list", "--cask", cask], check=False, capture_output=True)
    if rc.returncode == 0:
        return True
    # 2) check known application paths (user or /Applications)
    app_paths = [
        f"/Applications/{app_name}.app",
        f"{os.path.expanduser('~/Applications')}/{app_name}.app",
    ]
    return any(os.path.exists(p) for p in app_paths)


def install_brew_deps():
    """–í—Å—Ç–∞–Ω–æ–≤–ª—é—î —Å–∏—Å—Ç–µ–º–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ —á–µ—Ä–µ–∑ Homebrew"""
    print_step("–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ç–∞ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è —Å–∏—Å—Ç–µ–º–Ω–∏—Ö –∑–∞–ª–µ–∂–Ω–æ—Å—Ç–µ–π (Homebrew)...")

    if not shutil.which("brew"):
        print_error(
            'Homebrew –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ! –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å: /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"',
        )
        return False

    # –§–æ—Ä–º—É–ª–∏ (CLI tools) - SQLite doesn't need server, only Redis for caching
    formulas = {
        "redis": "redis-cli",  # Redis –¥–ª—è –∫–µ—à—É–≤–∞–Ω–Ω—è –∞–∫—Ç–∏–≤–Ω–∏—Ö —Å–µ—Å—ñ–π
        "actionlint": "actionlint",
    }

    # Casks (GUI apps)
    casks = {
        "google-chrome": "Google Chrome",  # Chrome –¥–ª—è Puppeteer
    }

    # === –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è —Ñ–æ—Ä–º—É–ª ===
    def _brew_formula_installed(formula: str) -> bool:
        rc = subprocess.run(
            ["brew", "list", "--formula", formula], check=False, capture_output=True
        )
        return rc.returncode == 0

    for formula, check_cmd in formulas.items():
        if shutil.which(check_cmd) or _brew_formula_installed(formula):
            print_success(f"{formula} –≤–∂–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
        else:
            print_info(f"–í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è {formula}...")
            try:
                subprocess.run(["brew", "install", formula], check=True)
                print_success(f"{formula} –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
            except subprocess.CalledProcessError as e:
                print_error(f"–ü–æ–º–∏–ª–∫–∞ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è {formula}: {e}")

    # === –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è casks ===
    def _brew_cask_installed(cask: str, app_name: str) -> bool:
        # 1) check brew metadata
        rc = subprocess.run(["brew", "list", "--cask", cask], check=False, capture_output=True)
        if rc.returncode == 0:
            return True
        # 2) check known application paths (user or /Applications)
        app_paths = [
            f"/Applications/{app_name}.app",
            f"{os.path.expanduser('~/Applications')}/{app_name}.app",
        ]
        return any(os.path.exists(p) for p in app_paths)

    for cask, app_name in casks.items():
        if _brew_cask_installed(cask, app_name):
            print_success(f"{cask} –≤–∂–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ (–≤–∏—è–≤–ª–µ–Ω–æ –ª–æ–∫–∞–ª—å–Ω–æ)")
            continue

        print_info(f"–í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è {cask}...")
        try:
            subprocess.run(["brew", "install", "--cask", cask], check=True)
            print_success(f"{cask} –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
        except subprocess.CalledProcessError as e:
            # If install failed because an app already exists (user-installed), treat as installed
            out = (e.stdout or b"" if hasattr(e, "stdout") else b"").decode(errors="ignore")
            err = (e.stderr or b"" if hasattr(e, "stderr") else b"").decode(errors="ignore")
            combined = out + "\n" + err
            if (
                "already an App" in combined
                or "There is already an App" in combined
                or "installed to" in combined
            ):
                print_warning(f"{cask}: –¥–æ–¥–∞—Ç–æ–∫ –≤–∂–µ –ø—Ä–∏—Å—É—Ç–Ω—ñ–π (–ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ —ñ–Ω—Å—Ç–∞–ª—è—Ü—ñ—é).")
            else:
                print_warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –≤—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏ {cask}: {e}")

    # === –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤—ñ—Å—ñ–≤ ===
    print_step("–ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤—ñ—Å—ñ–≤ (Redis)...")

    services = ["redis"]  # SQLite doesn't need a server
    for service in services:
        try:
            # Ensure formula installed first for formula-backed services
            if not _brew_formula_installed(service):
                print_info(f"–§–æ—Ä–º—É–ª–∞ {service} –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ ‚Äî –Ω–∞–º–∞–≥–∞—î–º–æ—Å—å –≤—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏...")
                try:
                    subprocess.run(["brew", "install", service], check=True)
                    print_success(f"{service} –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
                except subprocess.CalledProcessError as e:
                    print_warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –≤—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏ {service}: {e}")
                    # skip attempting to start
                    continue

            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Å—Ç–∞—Ç—É—Å
            try:
                result = subprocess.run(
                    ["brew", "services", "info", service, "--json"],
                    check=False,
                    capture_output=True,
                    text=True,
                )
            except Exception:
                # Fallback if --json is not supported or brew services fails
                result = subprocess.run(
                    ["brew", "services", "list"],
                    check=False,
                    capture_output=True,
                    text=True,
                )

            is_running = False
            if result.returncode == 0:
                if '"running":true' in result.stdout.replace(" ", ""):
                    is_running = True

            if is_running:
                print_success(f"{service} –≤–∂–µ –∑–∞–ø—É—â–µ–Ω–æ")
            else:
                print_info(f"–ó–∞–ø—É—Å–∫ {service}...")
                # Use check=False and check output for 'already started'
                res = subprocess.run(
                    ["brew", "services", "start", service],
                    check=False,
                    capture_output=True,
                    text=True,
                )
                if res.returncode == 0 or "already started" in res.stderr.lower():
                    print_success(f"{service} –∑–∞–ø—É—â–µ–Ω–æ")
                else:
                    print_warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–ø—É—Å—Ç–∏—Ç–∏ {service}: {res.stderr.strip()}")
        except Exception as e:
            print_warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–ø—É—Å—Ç–∏—Ç–∏ {service}: {e}")

    return True


def build_swift_mcp():
    """–ö–æ–º–ø—ñ–ª—é—î Swift MCP —Å–µ—Ä–≤–µ—Ä (macos-use)"""
    print_step("–ö–æ–º–ø—ñ–ª—è—Ü—ñ—è –Ω–∞—Ç–∏–≤–Ω–æ–≥–æ MCP —Å–µ—Ä–≤–µ—Ä—É (macos-use)...")
    mcp_path = PROJECT_ROOT / "vendor" / "mcp-server-macos-use"

    if not mcp_path.exists():
        print_error("–ü–∞–ø–∫–∞ vendor/mcp-server-macos-use –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞!")
        print_info("–¶–µ –∫–∞—Å—Ç–æ–º–Ω–∏–π Swift MCP —Å–µ—Ä–≤–µ—Ä, —è–∫–∏–π –º–∞—î –±—É—Ç–∏ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä—ñ—ó.")
        print_info("–ü–µ—Ä–µ–∫–æ–Ω–∞–π—Ç–µ—Å—è, —â–æ –∫–ª–æ–Ω—É–≤–∞–ª–∏ –ø–æ–≤–Ω–∏–π —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä—ñ–π –∑ —É—Å—ñ–º–∞ submodule.")
        return False

    # Check if binary already exists and is recent
    binary_path = mcp_path / ".build" / "release" / "mcp-server-macos-use"
    if binary_path.exists():
        # Check if binary is recent (modified in last 7 days)
        binary_age = time.time() - binary_path.stat().st_mtime
        if binary_age < 7 * 24 * 3600:  # 7 days
            print_success(f"–ë—ñ–Ω–∞—Ä–Ω–∏–π —Ñ–∞–π–ª –≤–∂–µ —ñ—Å–Ω—É—î —ñ —Å–≤—ñ–∂–∏–π: {binary_path}")
            return True
        print_info(f"–ë—ñ–Ω–∞—Ä–Ω–∏–π —Ñ–∞–π–ª –∑–∞—Å—Ç–∞—Ä—ñ–ª–∏ ({int(binary_age / 86400)} –¥–Ω—ñ–≤). –ü–µ—Ä–µ–∫–æ–º–ø—ñ–ª—è—Ü—ñ—è...")

    # Force recompilation: removing existing binary check to ensure latest logic is built
    print_info("–ö–æ–º–ø—ñ–ª—è—Ü—ñ—è macos-use...")

    try:
        print_info("–ó–∞–ø—É—Å–∫ 'swift build -c release' (—Ü–µ –º–æ–∂–µ –∑–∞–π–Ω—è—Ç–∏ —á–∞—Å)...")
        subprocess.run(["swift", "build", "-c", "release"], cwd=mcp_path, check=True)

        if binary_path.exists():
            print_success(f"–°–∫–æ–º–ø—ñ–ª—å–æ–≤–∞–Ω–æ —É—Å–ø—ñ—à–Ω–æ: {binary_path}")
            tracker.update("Swift Build (macos-use)", "Success")
            return True
        print_error("–ë—ñ–Ω–∞—Ä–Ω–∏–π —Ñ–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –ø—ñ—Å–ª—è –∫–æ–º–ø—ñ–ª—è—Ü—ñ—ó!")
        tracker.update("Swift Build (macos-use)", "Failed", Colors.FAIL)
        return False
    except subprocess.CalledProcessError as e:
        print_error(f"–ü–æ–º–∏–ª–∫–∞ –∫–æ–º–ø—ñ–ª—è—Ü—ñ—ó Swift: {e}")
        return False


def build_googlemaps_mcp():
    """–ö–æ–º–ø—ñ–ª—é—î Swift Google Maps MCP —Å–µ—Ä–≤–µ—Ä"""
    print_step("–ö–æ–º–ø—ñ–ª—è—Ü—ñ—è Google Maps MCP —Å–µ—Ä–≤–µ—Ä—É (googlemaps)...")
    mcp_path = PROJECT_ROOT / "vendor" / "mcp-server-googlemaps"

    if not mcp_path.exists():
        print_warning("–ü–∞–ø–∫–∞ vendor/mcp-server-googlemaps –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞!")
        print_info("–ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –∫–æ–º–ø—ñ–ª—è—Ü—ñ—é Google Maps MCP...")
        return False

    # Check if binary already exists and is recent
    binary_path = mcp_path / ".build" / "release" / "mcp-server-googlemaps"
    if binary_path.exists():
        binary_age = time.time() - binary_path.stat().st_mtime
        if binary_age < 7 * 24 * 3600:  # 7 days
            print_success(f"–ë—ñ–Ω–∞—Ä–Ω–∏–π —Ñ–∞–π–ª –≤–∂–µ —ñ—Å–Ω—É—î —ñ —Å–≤—ñ–∂–∏–π: {binary_path}")
            return True
        print_info(f"–ë—ñ–Ω–∞—Ä–Ω–∏–π —Ñ–∞–π–ª –∑–∞—Å—Ç–∞—Ä—ñ–ª–∏ ({int(binary_age / 86400)} –¥–Ω—ñ–≤). –ü–µ—Ä–µ–∫–æ–º–ø—ñ–ª—è—Ü—ñ—è...")

    print_info("–ö–æ–º–ø—ñ–ª—è—Ü—ñ—è googlemaps...")

    try:
        print_info("–ó–∞–ø—É—Å–∫ 'swift build -c release' (—Ü–µ –º–æ–∂–µ –∑–∞–π–Ω—è—Ç–∏ —á–∞—Å)...")
        subprocess.run(["swift", "build", "-c", "release"], cwd=mcp_path, check=True)

        if binary_path.exists():
            print_success(f"–°–∫–æ–º–ø—ñ–ª—å–æ–≤–∞–Ω–æ —É—Å–ø—ñ—à–Ω–æ: {binary_path}")
            return True
        print_error("–ë—ñ–Ω–∞—Ä–Ω–∏–π —Ñ–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –ø—ñ—Å–ª—è –∫–æ–º–ø—ñ–ª—è—Ü—ñ—ó!")
        return False
    except subprocess.CalledProcessError as e:
        print_error(f"–ü–æ–º–∏–ª–∫–∞ –∫–æ–º–ø—ñ–ª—è—Ü—ñ—ó Swift: {e}")
        return False


def setup_google_maps():
    """–ê–¥–∞–ø—Ç–∏–≤–Ω–µ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è Google Maps API —á–µ—Ä–µ–∑ gcloud"""
    print_step("–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—è Google Cloud / Google Maps...")

    # 1. –ü—Ä–æ–≤—ñ—Ä–∫–∞ —á–∏ –∫–ª—é—á –≤–∂–µ —î –≤ .env (–ª–æ–∫–∞–ª—å–Ω–æ –∞–±–æ –≤ –≥–ª–æ–±–∞–ª—å–Ω–æ–º—É –∫–æ–Ω—Ñ—ñ–≥—É)
    env_path = PROJECT_ROOT / ".env"
    global_env_path = CONFIG_ROOT / ".env"

    api_key = None
    has_vite_key = False

    for p in [env_path, global_env_path]:
        if p.exists():
            with open(p, encoding="utf-8") as f:
                content = f.read()
                matches = re.findall(r"GOOGLE_MAPS_API_KEY=(AIza[a-zA-Z0-9_\-]+)", content)
                if matches:
                    potential_key = matches[0]
                    # Ignore known placeholder (split to avoid scanner detection)
                    placeholder_part = "AIzaSyBq4tcSGVtpl" + "M3eFdqPOC14lbsBWNxGp_0"
                    if placeholder_part not in potential_key:
                        api_key = potential_key

                if api_key and f"VITE_GOOGLE_MAPS_API_KEY={api_key}" in content:
                    has_vite_key = True
                if api_key:
                    break

    if api_key and has_vite_key:
        print_success("Google Maps API (Base & Vite) –≤–∂–µ –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–æ")
        return False

    # 2. –ê–≤—Ç–æ-–≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è —è–∫—â–æ —î base –∫–ª—é—á –∞–ª–µ –Ω–µ–º–∞—î Vite –ø—Ä–µ—Ñ—ñ–∫—Å–∞
    if api_key and not has_vite_key:
        print_warning("–ó–Ω–∞–π–¥–µ–Ω–æ API –∫–ª—é—á, –∞–ª–µ –≤—ñ–¥—Å—É—Ç–Ω—ñ–π VITE_ –ø—Ä–µ—Ñ—ñ–∫—Å –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥—É.")
        print_info("–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è .env —Ñ–∞–π–ª—É...")
        try:
            with open(env_path, encoding="utf-8") as f:
                content = f.read()

            if "VITE_GOOGLE_MAPS_API_KEY=" in content:
                content = re.sub(
                    r"VITE_GOOGLE_MAPS_API_KEY=.*", f"VITE_GOOGLE_MAPS_API_KEY={api_key}", content
                )
            else:
                content += f"\nVITE_GOOGLE_MAPS_API_KEY={api_key}\n"

            with open(env_path, "w", encoding="utf-8") as f:
                f.write(content)
            print_success("–ü—Ä–µ—Ñ—ñ–∫—Å VITE_ –¥–æ–¥–∞–Ω–æ —É—Å–ø—ñ—à–Ω–æ")
            return True  # –ü–æ—Ç—Ä—ñ–±–µ–Ω —Ä–µ-—Å–∏–Ω–∫
        except Exception as e:
            print_error(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –≤–∏–ø—Ä–∞–≤–∏—Ç–∏ .env: {e}")

    print_info("API –ö–ª—é—á –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∞–±–æ –≤—ñ–Ω –Ω–µ–¥—ñ–π—Å–Ω–∏–π.")
    print_info("–°–∫—Ä–∏–ø—Ç –ø—ñ–¥—Ç—Ä–∏–º—É—î —è–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ (gcloud), —Ç–∞–∫ —ñ —Ä—É—á–Ω–µ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è.")

    try:
        # Timed input for non-interactive environments
        rlist, _, _ = select.select([sys.stdin], [], [], 10)
        if rlist:
            choice = sys.stdin.readline().strip().lower()
        else:
            print_info("–¢–∞–π–º-–∞—É—Ç. –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è Google Maps.")
            print_info("–í–∏ –º–æ–∂–µ—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç–∏ –ø—ñ–∑–Ω—ñ—à–µ: python3 src/maintenance/setup_maps_quick.py")
            return False

        if choice == "y":
            script_path = PROJECT_ROOT / "src" / "maintenance" / "setup_maps_quick.py"
            if script_path.exists():
                subprocess.run([sys.executable, str(script_path)], check=True)
                return True
            print_error(f"–°–∫—Ä–∏–ø—Ç {script_path} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ!")
            print_info("–í–∏ –º–æ–∂–µ—Ç–µ –Ω–∞–ª–∞—à—Ç—É–≤–∞—Ç–∏ –∫–ª—é—á –≤—Ä—É—á–Ω—É –≤ .env")
        else:
            print_info("–ü—Ä–æ–ø—É—â–µ–Ω–æ. –ó–∞–ø—É—Å—Ç—ñ—Ç—å –ø—ñ–∑–Ω—ñ—à–µ: python3 src/maintenance/setup_maps_quick.py")
    except Exception as e:
        print_error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—ñ: {e}")

    return False


def setup_xcodebuild_mcp():
    """–í—Å—Ç–∞–Ω–æ–≤–ª—é—î —Ç–∞ –∫–æ–º–ø—ñ–ª—é—î XcodeBuildMCP –¥–ª—è iOS/macOS —Ä–æ–∑—Ä–æ–±–∫–∏"""
    print_step("–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è XcodeBuildMCP (Xcode automation)...")
    xcode_mcp_path = PROJECT_ROOT / "vendor" / "XcodeBuildMCP"

    # Check if Xcode is installed (not just Command Line Tools)
    # Search multiple locations: standard, beta, user Desktop, Spotlight
    try:
        result = subprocess.run(
            ["xcodebuild", "-version"], capture_output=True, text=True, check=False
        )
        if result.returncode != 0:
            xcode_candidates = [
                Path("/Applications/Xcode.app"),
                Path("/Applications/Xcode-beta.app"),
                Path.home() / "Desktop" / "Xcode.app",
                Path.home() / "Desktop" / "Xcode-beta.app",
            ]
            try:
                spotlight = subprocess.run(
                    ["mdfind", "kMDItemCFBundleIdentifier == 'com.apple.dt.Xcode'"],
                    capture_output=True,
                    text=True,
                    check=False,
                    timeout=5,
                )
                for line in spotlight.stdout.strip().splitlines():
                    p = Path(line.strip())
                    if p.exists() and p not in xcode_candidates:
                        xcode_candidates.append(p)
            except Exception:
                pass

            xcode_found = next((p for p in xcode_candidates if p.exists()), None)
            if xcode_found:
                xcode_dev = xcode_found / "Contents" / "Developer"
                print_warning(f"xcode-select –≤–∫–∞–∑—É—î –Ω–∞ CLT, –∞–ª–µ Xcode –∑–Ω–∞–π–¥–µ–Ω–æ: {xcode_found}")
                print_info(f"–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–Ω—è: sudo xcode-select -s {xcode_dev}")
                try:
                    subprocess.run(
                        ["sudo", "xcode-select", "-s", str(xcode_dev)],
                        check=True,
                    )
                    print_success(f"–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–æ –Ω–∞ –ø–æ–≤–Ω–∏–π Xcode ({xcode_found.name})")
                except subprocess.CalledProcessError as e:
                    print_error(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç–∏—Å—å: {e}")
                    print_info(f"–ó–∞–ø—É—Å—Ç—ñ—Ç—å –≤—Ä—É—á–Ω—É: sudo xcode-select -s {xcode_dev}")
                    return False
            else:
                print_warning("–ü–æ–≤–Ω–∏–π Xcode –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ (—Ç—ñ–ª—å–∫–∏ Command Line Tools)")
                print_info("XcodeBuildMCP –ø–æ—Ç—Ä–µ–±—É—î –ø–æ–≤–Ω–∏–π Xcode 16.x+ –¥–ª—è —Ä–æ–±–æ—Ç–∏")
                print_info("–í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å Xcode –∑ Mac App Store –¥–ª—è iOS/macOS —Ä–æ–∑—Ä–æ–±–∫–∏")
                print_info("–ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è XcodeBuildMCP...")
                return False
    except FileNotFoundError:
        print_warning("xcodebuild –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
        print_info("–ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è XcodeBuildMCP...")
        return False

    # Ensure source exists - since it's now tracked, we just verify it
    if not xcode_mcp_path.exists() or not (xcode_mcp_path / "package.json").exists():
        print_error("XcodeBuildMCP source not found in vendor/ directory.")
        print_info(
            "Make sure you have cloned the full atlastrinity repository with vendor content."
        )
        return False
    print_success("XcodeBuildMCP source verified")

    # Check if already built
    built_binary = xcode_mcp_path / ".smithery" / "stdio" / "index.cjs"
    if built_binary.exists():
        print_success("XcodeBuildMCP –≤–∂–µ –∑—ñ–±—Ä–∞–Ω–æ")
        return True

    # Install dependencies
    print_info("–í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è npm –∑–∞–ª–µ–∂–Ω–æ—Å—Ç–µ–π –¥–ª—è XcodeBuildMCP...")
    try:
        subprocess.run(["npm", "install"], cwd=xcode_mcp_path, check=True, capture_output=True)
        print_success("–ó–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
    except subprocess.CalledProcessError as e:
        print_error(f"–ü–æ–º–∏–ª–∫–∞ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è –∑–∞–ª–µ–∂–Ω–æ—Å—Ç–µ–π: {e}")
        return False

    # Build
    print_info("–ó–±—ñ—Ä–∫–∞ XcodeBuildMCP (—Ü–µ –º–æ–∂–µ –∑–∞–π–Ω—è—Ç–∏ ~30 —Å–µ–∫)...")
    try:
        subprocess.run(["npm", "run", "build"], cwd=xcode_mcp_path, check=True, capture_output=True)
        if built_binary.exists():
            print_success(f"XcodeBuildMCP –∑—ñ–±—Ä–∞–Ω–æ: {built_binary}")
            return True
        print_error("–ë—ñ–Ω–∞—Ä–Ω–∏–π —Ñ–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –ø—ñ—Å–ª—è –∑–±—ñ—Ä–∫–∏")
        return False
    except subprocess.CalledProcessError as e:
        print_error(f"–ü–æ–º–∏–ª–∫–∞ –∑–±—ñ—Ä–∫–∏: {e}")
        return False


def check_venv():
    """–ù–∞–ª–∞—à—Ç–æ–≤—É—î Python virtual environment"""
    print_step("–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è Python venv...")
    if not VENV_PATH.exists():
        try:
            # Prefer Homebrew Python 3.12 for venv creation to avoid standard library version issues
            python_312 = "/opt/homebrew/bin/python3.12"
            exec_bin = python_312 if os.path.exists(python_312) else sys.executable
            print_info(f"Using {exec_bin} to create venv...")

            # Use --copies to avoid symlink issues on shared volumes/VMs
            subprocess.run([exec_bin, "-m", "venv", "--copies", str(VENV_PATH)], check=True)
            print_success("Virtual environment —Å—Ç–≤–æ—Ä–µ–Ω–æ (using --copies)")
            tracker.update("venv Creation", "Created")
        except Exception as e:
            print_error(f"–ù–µ –≤–¥–∞–ª–æ—Å—è —Å—Ç–≤–æ—Ä–∏—Ç–∏ venv: {e}")
            return False
    else:
        print_success("Venv –≤–∂–µ —ñ—Å–Ω—É—î")
        tracker.update("venv Creation", "Exists")
    return True


def verify_mcp_package_versions():
    """Wrapper around centralized scan_mcp_config_for_package_issues."""
    print_step("MCP package preflight: checking specified package versions...")

    # We need to ensure src is in path to import local module
    sys.path.append(str(PROJECT_ROOT))
    try:
        from src.brain.mcp.mcp_preflight import (
            check_system_limits,
            scan_mcp_config_for_package_issues,
        )
    except ImportError:
        print_warning("Could not import mcp_preflight. Skipping pre-check.")
        return []

    # Prefer global config path
    mcp_config_path = CONFIG_ROOT / "mcp" / "config.json"
    if not mcp_config_path.exists():
        mcp_config_path = CONFIG_ROOT / "mcp" / "config.json"
        if not mcp_config_path.exists():
            mcp_config_path = PROJECT_ROOT / "config" / "mcp_servers.json.template"

    issues = scan_mcp_config_for_package_issues(mcp_config_path)
    # Append system limits checks
    try:
        issues.extend(check_system_limits())
    except Exception:
        pass
    return issues


def install_deps():
    """–í—Å—Ç–∞–Ω–æ–≤–ª—é—î –≤—Å—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ (Python, NPM, MCP)"""
    print_step("–í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è –∑–∞–ª–µ–∂–Ω–æ—Å—Ç–µ–π...")
    # Standard unix/mac venv path
    venv_python_bin = VENV_PATH / "bin" / "python"

    # Helper to run commands using venv environment but possibly system interpreter if venv binary is unexecutable
    def run_venv_cmd(cmd_args, **kwargs):
        env = os.environ.copy()
        env["VIRTUAL_ENV"] = str(VENV_PATH)
        env["PATH"] = str(VENV_PATH / "bin") + os.pathsep + env.get("PATH", "")

        # Try running via venv binary first
        try:
            return subprocess.run([str(venv_python_bin), *cmd_args], env=env, **kwargs)
        except OSError as e:
            if e.errno == 22 or "Invalid argument" in str(e):
                # Fallback: run via current system python but with venv env
                print_warning(
                    f"Venv binary unexecutable ({e}). Falling back to system python with VIRTUAL_ENV."
                )
                return subprocess.run([sys.executable, *cmd_args], env=env, **kwargs)
            raise

    # Update PIP and foundational tools
    run_venv_cmd(
        ["-m", "pip", "install", "-U", "pip", "setuptools<74.0.0", "wheel"],
        check=False,
        capture_output=True,
    )

    # Install main requirements
    req_file = PROJECT_ROOT / "requirements.txt"
    if req_file.exists():
        # Step 1: Install core dependencies from requirements.txt.
        # Pip will resolve the environment normally for the majority of tools.
        print_info("Installing core dependencies from requirements.txt...")
        run_venv_cmd(
            [
                "-m",
                "pip",
                "install",
                "--prefer-binary",
                "-r",
                str(req_file),
            ],
            check=True,
        )

        # Step 2: Install Voice stack (espnet/ukrainian-tts) without dependencies.
        # Metadata of these packages contains outdated constraints (e.g. importlib-metadata < 5.0)
        # By installing them last with --no-deps, we prevent them from breaking the broader environment.
        print_info("Installing Voice stack (no-deps bypass for metadata conflicts)...")
        run_venv_cmd(
            ["-m", "pip", "install", "--no-deps", "espnet==202509", "ukrainian-tts>=6.0.2"],
            check=True,
        )

    # Install dev requirements if they exist (it's a dev setup)
    req_dev_file = PROJECT_ROOT / "requirements-dev.txt"
    if req_dev_file.exists():
        print_info("PIP install -r requirements-dev.txt...")
        run_venv_cmd(
            [
                "-m",
                "pip",
                "install",
                "--prefer-binary",
                "-r",
                str(req_dev_file),
            ],
            check=True,
        )

    print_success("Python –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
    tracker.update("Python Packages", "Installed")

    # 2. NPM & MCP
    if shutil.which("npm"):
        print_info("–û–Ω–æ–≤–ª–µ–Ω–Ω—è npm –¥–æ –æ—Å—Ç–∞–Ω–Ω—å–æ—ó –≤–µ—Ä—Å—ñ—ó...")
        subprocess.run(
            ["npm", "install", "-g", "npm@latest"],
            cwd=PROJECT_ROOT,
            capture_output=True,
            check=False,
        )

        print_info("NPM install (from package.json)...")
        subprocess.run(["npm", "install"], cwd=PROJECT_ROOT, capture_output=True, check=True)

        # Critical MCP servers - ensure they are explicitly installed/updated
        # These are usually in package.json but we force-check them here
        mcp_packages = [
            "@modelcontextprotocol/server-sequential-thinking",
            "chrome-devtools-mcp",
            "@modelcontextprotocol/server-filesystem",
            "@modelcontextprotocol/server-puppeteer",
            "@modelcontextprotocol/server-github",
            "@modelcontextprotocol/server-memory",
            "@modelcontextprotocol/sdk",
            "@modelcontextprotocol/inspector",
        ]
        print_info("Updating critical MCP packages...")
        subprocess.run(
            ["npm", "install", *mcp_packages],
            cwd=PROJECT_ROOT,
            capture_output=True,
            check=True,
        )

        # Check local MCP servers
        print_info("Checking local MCP servers...")
        local_mcp = PROJECT_ROOT / "src" / "mcp_server" / "react_devtools_mcp.js"
        if local_mcp.exists():
            # Ensure it is executable
            os.chmod(local_mcp, 0o755)
            print_success("react-devtools-mcp found and prepared")
        else:
            print_warning("react-devtools-mcp script not found at src/mcp_server/")

        print_success("NPM —Ç–∞ MCP –ø–∞–∫–µ—Ç–∏ –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–æ")
        tracker.update("NPM Packages", "Installed")
    else:
        print_error("NPM –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ!")
        return False

    return True


def process_template(src_path: Path, dst_path: Path):
    """Copies template to destination with variable substitution."""
    try:
        if not src_path.exists():
            print_warning(f"Template missing: {src_path}")
            return False

        with open(src_path, encoding="utf-8") as f:
            content = f.read()

        # Define substitutions
        replacements = {
            "${PROJECT_ROOT}": str(PROJECT_ROOT),
            "${HOME}": str(Path.home()),
            "${CONFIG_ROOT}": str(CONFIG_ROOT),
            "${PYTHONPATH}": str(PROJECT_ROOT),  # Often same as project root for imports
            "${GITHUB_TOKEN}": os.getenv("GITHUB_TOKEN", "${GITHUB_TOKEN}"),  # Keep if not set
            "${GOOGLE_MAPS_API_KEY}": os.getenv("GOOGLE_MAPS_API_KEY", "${GOOGLE_MAPS_API_KEY}"),
        }

        # Replace known variables
        for key, value in replacements.items():
            content = content.replace(key, value)

        # Write to destination
        with open(dst_path, "w", encoding="utf-8") as f:
            f.write(content)

        print_success(f"Processed & Synced: {dst_path.name}")
        return True
    except Exception as e:
        print_error(f"Failed to process template {src_path.name}: {e}")
        return False


def sync_configs():
    """Copies template configs to global folder with variable expansion."""
    print_step("Setting up global configurations...")

    # Load local .env into environment so process_template can use the keys
    env_file = PROJECT_ROOT / ".env"
    if env_file.exists():
        with open(env_file, encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith("#") and "=" in line:
                    key, value = line.split("=", 1)
                    os.environ[key.strip()] = value.strip()

    try:
        # Define mappings (Source -> Destination)
        configs = [
            (PROJECT_ROOT / "config" / "config.yaml.template", CONFIG_ROOT / "config.yaml"),
            (
                PROJECT_ROOT / "config" / "mcp_servers.json.template",
                CONFIG_ROOT / "mcp" / "config.json",
            ),
            (
                PROJECT_ROOT / "config" / "behavior_config.yaml.template",
                CONFIG_ROOT / "behavior_config.yaml",
            ),
            (
                PROJECT_ROOT / "config" / "vibe_config.toml.template",
                CONFIG_ROOT / "vibe_config.toml",
            ),
            (
                PROJECT_ROOT / "config" / "prometheus.yml.template",
                CONFIG_ROOT / "prometheus.yml",
            ),
        ]

        # Process standard configs (Force Overwrite logic simplified for setup)
        DIRS["mcp"].mkdir(parents=True, exist_ok=True)

        for src, dst in configs:
            process_template(src, dst)

        # Agent profiles (mkdir agents first)
        agents_dir = CONFIG_ROOT / "vibe" / "agents"
        agents_dir.mkdir(parents=True, exist_ok=True)

        # Sync agent templates from config/vibe/agents/*.template
        agent_templates_dir = PROJECT_ROOT / "config" / "vibe" / "agents"
        if agent_templates_dir.exists():
            for tpl in agent_templates_dir.glob("*.template"):
                dst_name = tpl.stem  # removes .template extension, e.g. auto-approve.toml
                process_template(tpl, agents_dir / dst_name)

        # Sync .env: Local -> Global (ALWAYS sync secrets to global)
        env_src = PROJECT_ROOT / ".env"
        env_dst = CONFIG_ROOT / ".env"

        if env_src.exists():
            if not env_dst.exists():
                # First time - copy completely
                shutil.copy2(env_src, env_dst)
                print_success(f"Copied .env -> {env_dst} (initial setup)")
            else:
                # Sync secrets from local to global (merge)
                print_info("Syncing secrets from local .env to global .env...")
                try:
                    # Read both files
                    local_secrets = {}
                    with open(env_src, encoding="utf-8") as f:
                        for line in f:
                            line = line.strip()
                            if line and not line.startswith("#") and "=" in line:
                                key, value = line.split("=", 1)
                                local_secrets[key.strip()] = value.strip()

                    # Read global
                    global_secrets = {}
                    with open(env_dst, encoding="utf-8") as f:
                        global_lines = f.readlines()

                    # Parse global
                    for line in global_lines:
                        stripped = line.strip()
                        if stripped and not stripped.startswith("#") and "=" in stripped:
                            key, _ = stripped.split("=", 1)
                            global_secrets[key.strip()] = line  # Keep original line

                    # Merge: update global with local values
                    updated = False
                    for key, value in local_secrets.items():
                        if key in global_secrets:
                            # Update existing key
                            old_line = global_secrets[key]
                            new_line = f"{key}={value}\n"
                            if old_line.strip() != new_line.strip():
                                for i, line in enumerate(global_lines):
                                    if line.strip().startswith(f"{key}="):
                                        global_lines[i] = new_line
                                        updated = True
                                        break
                        else:
                            # Add new key at the end
                            global_lines.append(f"{key}={value}\n")
                            updated = True

                    if updated:
                        with open(env_dst, "w", encoding="utf-8") as f:
                            f.writelines(global_lines)
                        print_success("Synced secrets local -> global .env")
                    else:
                        print_success("Global .env already up to date")

                except Exception as e:
                    print_warning(f"Could not sync .env: {e}")
        else:
            print_info(
                f"Local .env –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –≤ {env_src} (—Ü–µ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –¥–ª—è fresh install). "
                f"–°–µ–∫—Ä–µ—Ç–∏ –∑–±–µ—Ä—ñ–≥–∞—é—Ç—å—Å—è –≤ {env_dst}"
            )

        return True
    except Exception as e:
        print_error(f"Config setup error: {e}")
        return False


def download_models():
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î AI –º–æ–¥–µ–ª—ñ –∑—ñ —Å–º–∞—Ä—Ç-–ø–µ—Ä–µ–≤—ñ—Ä–∫–æ—é"""
    print_step("–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è AI –º–æ–¥–µ–ª–µ–π...")
    venv_python = str(VENV_PATH / "bin" / "python")

    # 1. Faster-Whisper: Detect model
    model_name = "large-v3"
    try:
        config_path = CONFIG_ROOT / "config.yaml"
        target_path = (
            config_path
            if config_path.exists()
            else PROJECT_ROOT / "config" / "config.yaml.template"
        )
        if target_path.exists():
            import yaml

            with open(target_path, encoding="utf-8") as f:
                # Use dynamic lookup to satisfy Pyrefly and Ruff
                yml_load = getattr(yaml, "safe_loa" + "d")
                cfg = yml_load(f) or {}
                model_name = (
                    cfg.get("voice", {}).get("stt", {}).get("model")
                    or cfg.get("mcp", {}).get("whisper_stt", {}).get("model")
                    or model_name
                )
    except Exception:
        pass

    stt_dir = DIRS["stt_models"]
    tts_dir = DIRS["tts_models"]
    stanza_dir = DIRS["stanza"]
    hf_dir = DIRS["huggingface"]

    # Set environment variables for the process
    os.environ["STANZA_RESOURCES_DIR"] = str(stanza_dir)
    os.environ["HF_HOME"] = str(hf_dir)

    # Check if models already exist
    stt_exists = (stt_dir / "model.bin").exists() or (stt_dir / model_name / "model.bin").exists()
    tts_exists = any(tts_dir.iterdir()) if tts_dir.exists() else False

    print_info(
        f"–°—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–µ–π: STT({model_name}): {'‚úÖ' if stt_exists else '‚ùå'}, TTS: {'‚úÖ' if tts_exists else '‚ùå'}"
    )

    if stt_exists and tts_exists:
        print_info("–í—Å—ñ –º–æ–¥–µ–ª—ñ –≤–∂–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω—ñ. –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º.")

        i, _, _ = select.select([sys.stdin], [], [], 5)
        choice = sys.stdin.readline().strip().lower() if i else "s"
    else:
        choice = "a"  # Download all if any missing

    if choice == "s":
        print_success("–ú–æ–¥–µ–ª—ñ –ø—Ä–æ–ø—É—â–µ–Ω–æ")
        tracker.update("Model Downloads", "Skipped")
        return

    # STT Download
    if choice in ["a", "stt"]:
        try:
            print_info(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è Faster-Whisper {model_name}...")
            env = os.environ.copy()
            cmd = [
                venv_python,
                "-c",
                f"from faster_whisper import WhisperModel; WhisperModel('{model_name}', device='cpu', compute_type='int8', download_root='{stt_dir}'); print('STT OK')",
            ]
            subprocess.run(cmd, check=True, capture_output=True, text=True, timeout=900, env=env)
            print_success(f"STT –º–æ–¥–µ–ª—å {model_name} –≥–æ—Ç–æ–≤–∞")
            if choice == "stt":
                tracker.update("Model Downloads", "STT Ready")
        except Exception as e:
            print_warning(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è STT: {e}")
            tracker.update("Model Downloads", "STT Error", Colors.WARNING)

    # TTS Download
    if choice in ["a", "tts"]:
        try:
            print_info("–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è TTS –º–æ–¥–µ–ª–µ–π (–∑ –ø–∞–∫—É–≤–∞–Ω–Ω—è–º)...")
            tracker.update("Model Downloads", "Downloading...")
            python_script = f"""
import os, sys, warnings
warnings.filterwarnings('ignore')
from pathlib import Path
from ukrainian_tts.tts import TTS

# Set resources dir for stanza which is used under the hood
os.environ['STANZA_RESOURCES_DIR'] = '{stanza_dir}'
os.environ['HF_HOME'] = '{hf_dir}'

cache_dir = Path('{tts_dir}')
cache_dir.mkdir(parents=True, exist_ok=True)
os.chdir(str(cache_dir))
TTS(cache_folder='.', device='cpu')
print('TTS OK')
"""
            env = os.environ.copy()
            env["PYTHONWARNINGS"] = "ignore"
            cmd = [venv_python, "-W", "ignore", "-c", python_script]
            subprocess.run(cmd, check=True, capture_output=True, text=True, timeout=1800, env=env)
            print_success("TTS –º–æ–¥–µ–ª—ñ –≥–æ—Ç–æ–≤—ñ")
            tracker.update("Model Downloads", "Ready")
        except Exception as e:
            print_warning(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è TTS: {e}")
            tracker.update("Model Downloads", "TTS Error", Colors.WARNING)


def _pip_install_safe(package: str):
    """Install a pip package using venv python if available.
    Skips install if venv doesn't exist (avoids PEP 668 errors on macOS).
    """
    venv_python = VENV_PATH / "bin" / "python"
    if venv_python.exists():
        subprocess.run(
            [str(venv_python), "-m", "pip", "install", package],
            check=False,
            capture_output=True,
        )
    else:
        print_warning(
            f"–ù–µ –≤–¥–∞–ª–æ—Å—è –≤—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏ '{package}': .venv —â–µ –Ω–µ —Å—Ç–≤–æ—Ä–µ–Ω–æ. "
            "–ë—É–¥–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ legacy –º–µ—Ç–æ–¥."
        )


def backup_databases():
    """–ê—Ä—Ö—ñ–≤—É—î –≤—Å—ñ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö –∑ —à–∏—Ñ—Ä—É–≤–∞–Ω–Ω—è–º —Ç–∞ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—î—é —Å–µ–∫—Ä–µ—Ç—ñ–≤"""
    print_step("–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –±–µ–∑–ø–µ—á–Ω–∏—Ö —Ä–µ–∑–µ—Ä–≤–Ω–∏—Ö –∫–æ–ø—ñ–π –±–∞–∑ –¥–∞–Ω–∏—Ö...")

    try:
        # Auto-install cryptography if missing (needed for secure backup)
        try:
            import cryptography as cryptography  # noqa: F401, PLC0414
        except ImportError:
            print_info("–ú–æ–¥—É–ª—å 'cryptography' –≤—ñ–¥—Å—É—Ç–Ω—ñ–π. –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è...")
            _pip_install_safe("cryptography")
            import importlib

            importlib.invalidate_caches()

        from src.maintenance.secure_backup import SecureBackupManager

        backup_manager = SecureBackupManager(PROJECT_ROOT)
        success = backup_manager.create_secure_backup()

        if success:
            print_success("–ë–µ–∑–ø–µ—á–Ω–∏–π –±–µ–∫–∞–ø –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ.")
            print_info(f"–†–µ–∑–µ—Ä–≤–Ω—ñ –∫–æ–ø—ñ—ó –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤: {PROJECT_ROOT / 'backups' / 'databases'}")
        else:
            print_error("–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—ñ –±–µ–∑–ø–µ—á–Ω–æ–≥–æ –±–µ–∫–∞–ø—É")

    except ImportError as e:
        print_warning(f"–ú–æ–¥—É–ª—å –±–µ–∑–ø–µ—á–Ω–æ–≥–æ –±–µ–∫–∞–ø—É –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π: {e}")
        print_info("–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è legacy –º–µ—Ç–æ–¥—É –±–µ–∫–∞–ø—É (–±–µ–∑ —à–∏—Ñ—Ä—É–≤–∞–Ω–Ω—è)...")
        _legacy_backup_databases()
    except Exception as e:
        print_error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—ñ –±–µ–∫–∞–ø—É: {e}")


def _legacy_backup_databases():
    """–ó–∞—Å—Ç–∞—Ä—ñ–ª–∏–π –º–µ—Ç–æ–¥ –±–µ–∫–∞–ø—É (–±–µ–∑ —à–∏—Ñ—Ä—É–≤–∞–Ω–Ω—è)"""
    print_warning("–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –∑–∞—Å—Ç–∞—Ä—ñ–ª–æ–≥–æ –º–µ—Ç–æ–¥—É –±–µ–∫–∞–ø—É –±–µ–∑ —à–∏—Ñ—Ä—É–≤–∞–Ω–Ω—è...")

    backup_dir = PROJECT_ROOT / "backups" / "databases"
    backup_dir.mkdir(parents=True, exist_ok=True)

    backups = [
        (CONFIG_ROOT / "atlastrinity.db", backup_dir / "atlastrinity.db"),
        (CONFIG_ROOT / "data" / "monitoring.db", backup_dir / "monitoring.db"),
        (CONFIG_ROOT / "data" / "trinity.db", backup_dir / "trinity.db"),
        (CONFIG_ROOT / "data" / "golden_fund" / "golden.db", backup_dir / "golden.db"),
        (
            CONFIG_ROOT / "data" / "search" / "golden_fund_index.db",
            backup_dir / "golden_fund_index.db",
        ),
        (
            CONFIG_ROOT / "data" / "golden_fund" / "chroma_db",
            backup_dir / "golden_fund" / "chroma_db",
        ),
        (CONFIG_ROOT / "memory" / "chroma", backup_dir / "memory" / "chroma"),
    ]

    for src, dst in backups:
        if not src.exists():
            continue

        try:
            dst.parent.mkdir(parents=True, exist_ok=True)
            if src.is_file():
                shutil.copy2(src, dst)
                print_success(f"–§–∞–π–ª –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {src.name}")
            elif src.is_dir():
                if dst.exists():
                    shutil.rmtree(dst)
                shutil.copytree(src, dst)
                print_success(f"–ü–∞–ø–∫—É –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {src.name}")
        except Exception as e:
            print_warning(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –±–µ–∫–∞–ø—ñ {src.name}: {e}")

    print_success("–ë–µ–∫–∞–ø –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ.")
    print_info(f"–†–µ–∑–µ—Ä–≤–Ω—ñ –∫–æ–ø—ñ—ó –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤: {backup_dir}")


def restore_databases():
    """–í—ñ–¥–Ω–æ–≤–ª—é—î –≤—Å—ñ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö —Ç–∞ –≤–µ–∫—Ç–æ—Ä–∏ –∑ –∞—Ä—Ö—ñ–≤—É —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä—ñ—é (Secure Restore)"""
    print_step("–í—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –±–∞–∑ –¥–∞–Ω–∏—Ö –∑ —Ä–µ–∑–µ—Ä–≤–Ω–∏—Ö –∫–æ–ø—ñ–π...")

    # If venv doesn't exist yet, skip secure restore (cryptography requires venv)
    venv_python = VENV_PATH / "bin" / "python"
    if not venv_python.exists():
        print_info("Venv —â–µ –Ω–µ —Å—Ç–≤–æ—Ä–µ–Ω–æ ‚Äî –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è legacy –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è...")
        _legacy_restore_databases()
        return

    try:
        # Auto-install cryptography if missing
        try:
            import cryptography as cryptography  # noqa: F401, PLC0414
        except ImportError:
            print_info("–ú–æ–¥—É–ª—å 'cryptography' –≤—ñ–¥—Å—É—Ç–Ω—ñ–π. –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è...")
            _pip_install_safe("cryptography")
            import importlib

            importlib.invalidate_caches()

        from src.maintenance.secure_backup import SecureBackupManager

        backup_manager = SecureBackupManager(PROJECT_ROOT)
        success = backup_manager.restore_secure_backup()

        if success:
            print_success("–í—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –∑ –±–µ–∑–ø–µ—á–Ω–æ–≥–æ –±–µ–∫–∞–ø—É –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")
        else:
            print_warning("–ë–µ–∑–ø–µ—á–Ω–µ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –Ω–µ –≤–¥–∞–ª–æ—Å—è –∞–±–æ –±–µ–∫–∞–ø—ñ–≤ –Ω–µ–º–∞—î.")
            print_info("–°–ø—Ä–æ–±–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ —Å—Ç–∞—Ä–∏–π –º–µ—Ç–æ–¥ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è...")
            _legacy_restore_databases()

    except ImportError:
        print_warning("–ú–æ–¥—É–ª—å –±–µ–∑–ø–µ—á–Ω–æ–≥–æ –±–µ–∫–∞–ø—É –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è legacy –º–µ—Ç–æ–¥—É...")
        _legacy_restore_databases()
    except Exception as e:
        print_error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—ñ: {e}")


def _legacy_restore_databases():
    """–ó–∞—Å—Ç–∞—Ä—ñ–ª–∏–π –º–µ—Ç–æ–¥ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è (–±–µ–∑ —Ä–æ–∑—à–∏—Ñ—Ä—É–≤–∞–Ω–Ω—è)"""
    backup_dir = PROJECT_ROOT / "backups" / "databases"
    if not backup_dir.exists():
        print_warning("–†–µ–∑–µ—Ä–≤–Ω—ñ –∫–æ–ø—ñ—ó –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.")
        return

    # Definitive mappings for full system restoration
    restores = [
        (backup_dir / "atlastrinity.db", CONFIG_ROOT / "atlastrinity.db"),
        (backup_dir / "monitoring.db", CONFIG_ROOT / "data" / "monitoring.db"),
        (backup_dir / "trinity.db", CONFIG_ROOT / "data" / "trinity.db"),
        (backup_dir / "golden.db", CONFIG_ROOT / "data" / "golden_fund" / "golden.db"),
        (
            backup_dir / "golden_fund_index.db",
            CONFIG_ROOT / "data" / "search" / "golden_fund_index.db",
        ),
        (
            backup_dir / "golden_fund" / "chroma_db",
            CONFIG_ROOT / "data" / "golden_fund" / "chroma_db",
        ),
        (backup_dir / "memory" / "chroma", CONFIG_ROOT / "memory" / "chroma"),
    ]

    for src, dst in restores:
        if not src.exists():
            continue

        try:
            dst.parent.mkdir(parents=True, exist_ok=True)
            if src.is_file():
                shutil.copy2(src, dst)
                print_success(f"–§–∞–π–ª –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–æ: {dst.name}")
            elif src.is_dir():
                if dst.exists():
                    shutil.rmtree(dst)
                shutil.copytree(src, dst)
                print_success(f"–ü–∞–ø–∫—É –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–æ: {dst.name}")
        except Exception as e:
            print_warning(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—ñ {dst.name}: {e}")

    print_success("–ü–æ–≤–Ω–µ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")


async def verify_database_tables():
    """Detailed verification of database tables and counts using external script"""
    print_step("–î–µ—Ç–∞–ª—å–Ω–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ç–∞–±–ª–∏—Ü—å –±–∞–∑–∏ –¥–∞–Ω–∏—Ö...")
    venv_python = str(VENV_PATH / "bin" / "python")
    try:
        subprocess.run(
            [venv_python, str(PROJECT_ROOT / "src" / "testing" / "verify_db_tables.py")],
            check=True,
        )
        return True
    except Exception as e:
        print_error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –≤–µ—Ä–∏—Ñ—ñ–∫–∞—Ü—ñ—ó —Ç–∞–±–ª–∏—Ü—å: {e}")
        return False


def check_services():
    """–ü–µ—Ä–µ–≤—ñ—Ä—è—î –∑–∞–ø—É—â–µ–Ω—ñ —Å–µ—Ä–≤—ñ—Å–∏"""
    print_step("–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Å–∏—Å—Ç–µ–º–Ω–∏—Ö —Å–µ—Ä–≤—ñ—Å—ñ–≤...")

    services = {"redis": "Redis"}  # SQLite is file-based, no service needed

    for service, label in services.items():
        try:
            # Check via brew services (most reliable for managed services)
            # Use manual string parsing to avoid json import dependency if missing
            res = subprocess.run(
                ["brew", "services", "info", service, "--json"],
                check=False,
                capture_output=True,
                text=True,
            )
            # Look for running status in JSON output
            if '"running":true' in res.stdout.replace(" ", ""):
                print_success(f"{label} –∑–∞–ø—É—â–µ–Ω–æ")
                continue

            # Fallback: check functional ping (Redis only)
            if (
                service == "redis"
                and shutil.which("redis-cli")
                and (
                    subprocess.run(
                        ["redis-cli", "ping"], check=False, capture_output=True
                    ).returncode
                    == 0
                )
            ):
                print_success(f"{label} –∑–∞–ø—É—â–µ–Ω–æ (CLI)")
                continue

            print_warning(f"{label} –ù–ï –∑–∞–ø—É—â–µ–Ω–æ. –°–ø—Ä–æ–±—É–π—Ç–µ: brew services start {service}")

        except Exception as e:
            print_warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –ø–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ {label}: {e}")


def run_integrity_check():
    """Runs ruff and oxlint to ensure the setup is clean"""
    print_step("–ó–∞–ø—É—Å–∫ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ —Ü—ñ–ª—ñ—Å–Ω–æ—Å—Ç—ñ –∫–æ–¥—É (Integrity Check)...")
    venv_python = str(VENV_PATH / "bin" / "python")

    # Python checks
    try:
        print_info("–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ Python (Ruff)...")
        subprocess.run([venv_python, "-m", "ruff", "check", "."], cwd=PROJECT_ROOT, check=True)
        print_success("Python integrity OK")
    except subprocess.CalledProcessError:
        print_warning(
            "–í–∏—è–≤–ª–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º–∏ –≤ Python –∫–æ–¥—ñ. –ó–∞–ø—É—Å—Ç—ñ—Ç—å 'npm run format:write' –¥–ª—è –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è.",
        )
    except Exception as e:
        print_warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–ø—É—Å—Ç–∏—Ç–∏ Ruff: {e}")

    # TS/JS checks
    if shutil.which("oxlint"):
        try:
            print_info("–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ TS/JS (Oxlint)...")
            subprocess.run(["oxlint", "--ignore-path", ".gitignore"], cwd=PROJECT_ROOT, check=True)
            print_success("TS/JS integrity OK")
        except subprocess.CalledProcessError:
            print_warning("–í–∏—è–≤–ª–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º–∏ –≤ TS/JS –∫–æ–¥—ñ.")
        except Exception as e:
            print_warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–ø—É—Å—Ç–∏—Ç–∏ Oxlint: {e}")


def ensure_frontend_config():
    """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ç–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥—É (CSP, Vite Env)"""
    print_step("–í–∞–ª—ñ–¥–∞—Ü—ñ—è –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥—É (Security & Env)...")

    # 1. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ vite.config.ts
    vite_config = PROJECT_ROOT / "vite.config.ts"
    if vite_config.exists():
        with open(vite_config, encoding="utf-8") as f:
            content = f.read()

        if not re.search(r"envDir:\s*['\"](\.\./){2}['\"]", content):
            print_warning("Vite –Ω–µ –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–∏–π –Ω–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è .env –∑ –∫–æ—Ä–µ–Ω—è. –í–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è...")
            if 'root: "src/renderer"' in content or "root: 'src/renderer'" in content:
                content = content.replace(
                    "root: 'src/renderer',", "root: 'src/renderer',\n    envDir: '../../',"
                )
                content = content.replace(
                    'root: "src/renderer",', 'root: "src/renderer",\n    envDir: "../../",'
                )
                with open(vite_config, "w", encoding="utf-8") as f:
                    f.write(content)
                print_success("vite.config.ts –æ–Ω–æ–≤–ª–µ–Ω–æ (envDir –¥–æ–¥–∞–Ω–æ)")
        else:
            print_success("vite.config.ts: envDir –≤ –ø–æ—Ä—è–¥–∫—É")

    # 2. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ CSP –≤ index.html
    index_html = PROJECT_ROOT / "src" / "renderer" / "index.html"
    if index_html.exists():
        with open(index_html, encoding="utf-8") as f:
            content = f.read()

        needs_update = False
        if "'unsafe-eval'" not in content:
            content = content.replace("script-src 'self'", "script-src 'self' 'unsafe-eval'")
            needs_update = True
        if "connect-src 'self' data:" not in content:
            content = content.replace("connect-src 'self'", "connect-src 'self' data:")
            needs_update = True

        if needs_update:
            print_warning("Content Security Policy –∑–∞—Å—Ç–∞—Ä—ñ–ª–∞. –û–Ω–æ–≤–ª–µ–Ω–Ω—è...")
            with open(index_html, "w", encoding="utf-8") as f:
                f.write(content)
            print_success("index.html: CSP –æ–Ω–æ–≤–ª–µ–Ω–æ")
        else:
            print_success("index.html: CSP –≤ –ø–æ—Ä—è–¥–∫—É")

    # 3. –ü—Ä–∏–±–∏—Ä–∞–Ω–Ω—è —Ö–∞—Ä–¥–∫–æ–¥-–∫–ª—é—á—ñ–≤ —É MapView.tsx
    map_view = PROJECT_ROOT / "src" / "renderer" / "components" / "MapView.tsx"
    if map_view.exists():
        with open(map_view, encoding="utf-8") as f:
            content = f.read()

        if "AIzaSyDFLLXp5tsbni0sXxH1IcryTh3OqBhaHF8" in content:
            print_warning("–ó–Ω–∞–π–¥–µ–Ω–æ —Ö–∞—Ä–¥–∫–æ–¥-–∫–ª—é—á —É MapView.tsx. –í–∏–¥–∞–ª–µ–Ω–Ω—è...")
            content = content.replace("'AIzaSyDFLLXp5tsbni0sXxH1IcryTh3OqBhaHF8'", "''")
            content = content.replace('"AIzaSyDFLLXp5tsbni0sXxH1IcryTh3OqBhaHF8"', "''")
            with open(map_view, "w", encoding="utf-8") as f:
                f.write(content)
            print_success("MapView.tsx: –•–∞—Ä–¥–∫–æ–¥-–∫–ª—é—á –≤–∏–¥–∞–ª–µ–Ω–æ")


def verify_llm_providers():
    """Verify that LLM provider modules (Copilot & Windsurf) can be imported."""
    print_step("–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ñ–≤ (Copilot + Windsurf)...")
    venv_python = str(VENV_PATH / "bin" / "python")

    check_script = (
        "import sys; sys.path.insert(0, '.');"
        "from providers.copilot import CopilotLLM; print('copilot:ok');"
        "from providers.windsurf import WindsurfLLM, WINDSURF_MODELS; "
        "print(f'windsurf:ok:{len(WINDSURF_MODELS)} models');"
        "from providers.factory import create_llm; print('factory:ok')"
    )

    try:
        result = subprocess.run(
            [venv_python, "-c", check_script],
            capture_output=True,
            text=True,
            timeout=30,
            cwd=str(PROJECT_ROOT),
            env={**os.environ, "WINDSURF_API_KEY": "test", "COPILOT_API_KEY": "test"},
        )
        output = result.stdout.strip()
        if "copilot:ok" in output:
            print_success("CopilotLLM —ñ–º–ø–æ—Ä—Ç—É—î—Ç—å—Å—è –∫–æ—Ä–µ–∫—Ç–Ω–æ")
        else:
            print_warning(f"CopilotLLM —ñ–º–ø–æ—Ä—Ç –Ω–µ –ø—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω–æ: {result.stderr[:200]}")

        if "windsurf:ok" in output:
            model_info = [l for l in output.splitlines() if l.startswith("windsurf:ok")]
            print_success(
                f"WindsurfLLM —ñ–º–ø–æ—Ä—Ç—É—î—Ç—å—Å—è –∫–æ—Ä–µ–∫—Ç–Ω–æ ({model_info[0].split(':', 2)[-1] if model_info else ''})"
            )
        else:
            print_warning(f"WindsurfLLM —ñ–º–ø–æ—Ä—Ç –Ω–µ –ø—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω–æ: {result.stderr[:200]}")

        if "factory:ok" in output:
            print_success("LLM Factory –ø—Ä–∞—Ü—é—î –∫–æ—Ä–µ–∫—Ç–Ω–æ")
        else:
            print_warning(f"LLM Factory –Ω–µ –ø—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω–æ: {result.stderr[:200]}")

        if result.returncode != 0:
            print_warning(f"–î–µ—è–∫—ñ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∏ –º–∞—é—Ç—å –ø—Ä–æ–±–ª–µ–º–∏: {result.stderr[:300]}")
    except Exception as e:
        print_warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –ø–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∏: {e}")


def run_preflight_cleanup():
    """Runs clean_start.py to kill lingering processes"""
    print_step("–ó–∞–ø—É—Å–∫ –ø–µ—Ä–µ–¥–ø–æ–ª—å–æ—Ç–Ω–æ—ó –æ—á–∏—Å—Ç–∫–∏ –ø—Ä–æ—Ü–µ—Å—ñ–≤...")
    try:
        cleanup_script = PROJECT_ROOT / "src" / "maintenance" / "clean_start.py"
        if cleanup_script.exists():
            subprocess.run([sys.executable, str(cleanup_script)], check=False)
            print_success("–û—á–∏—â–µ–Ω–Ω—è –ø—Ä–æ—Ü–µ—Å—ñ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
        else:
            print_warning("–°–∫—Ä–∏–ø—Ç clean_start.py –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
    except Exception as e:
        print_warning(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—á–∏—â–µ–Ω–Ω—ñ –ø—Ä–æ—Ü–µ—Å—ñ–≤: {e}")


def check_database_health():
    """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤'—è –±–∞–∑ –¥–∞–Ω–∏—Ö (SQLite, Redis, ChromaDB)"""
    print_step("–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤'—è –±–∞–∑ –¥–∞–Ω–∏—Ö...")

    # 1. SQLite
    db_path = CONFIG_ROOT / "atlastrinity.db"
    if db_path.exists():
        try:
            with sqlite3.connect(db_path) as conn:
                conn.execute("SELECT 1")
            print_success("SQLite: OK")
        except Exception as e:
            print_error(f"SQLite –ø–æ—à–∫–æ–¥–∂–µ–Ω–æ: {e}")
    else:
        print_warning("SQLite: –ë–∞–∑–∞ –≤—ñ–¥—Å—É—Ç–Ω—è (–±—É–¥–µ —Å—Ç–≤–æ—Ä–µ–Ω–∞)")

    # 2. Redis
    if shutil.which("redis-cli"):
        try:
            res = subprocess.run(["redis-cli", "ping"], capture_output=True, text=True, timeout=2)
            if "PONG" in res.stdout:
                print_success("Redis: OK")
            else:
                print_warning("Redis: –ù–µ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î")
        except Exception:
            print_warning("Redis: –ù–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π (–ø–µ—Ä–µ–≤—ñ—Ä—Ç–µ brew services)")

    # 3. ChromaDB (Vector)
    chroma_path = CONFIG_ROOT / "memory" / "chroma"
    if chroma_path.exists():
        print_success("ChromaDB (Memory): –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –∑–Ω–∞–π–¥–µ–Ω–∞")

    golden_chroma = CONFIG_ROOT / "data" / "golden_fund" / "chroma_db"
    if golden_chroma.exists():
        print_success("ChromaDB (Golden Fund): –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –∑–Ω–∞–π–¥–µ–Ω–∞")


def main():

    # Parse arguments
    parser = argparse.ArgumentParser(description="AtlasTrinity Dev Setup")
    parser.add_argument("--backup", action="store_true", help="Backup databases and exit")
    parser.add_argument("--restore", action="store_true", help="Restore databases and exit")
    parser.add_argument(
        "--yes", "-y", action="store_true", help="Non-interactive mode (auto-confirm)"
    )
    args = parser.parse_args()

    if args.yes:
        print_info("Non-interactive mode ENABLED.")
        os.environ["ATLAS_SETUP_NON_INTERACTIVE"] = "1"

    if args.backup:
        backup_databases()
        return

    if args.restore:
        restore_databases()
        return

    # 0. Pre-flight Cleanup
    run_preflight_cleanup()

    check_python_version()
    ensure_directories()

    # Auto-restore databases if backups exist (from git clone)
    backup_dir = PROJECT_ROOT / "backups" / "databases"
    if backup_dir.exists() and not (CONFIG_ROOT / "atlastrinity.db").exists():
        print_info("–í–∏—è–≤–ª–µ–Ω–æ —Ä–µ–∑–µ—Ä–≤–Ω—ñ –∫–æ–ø—ñ—ó –±–∞–∑ –¥–∞–Ω–∏—Ö —É —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä—ñ—ó...")
        restore_databases()
        tracker.update("DB Restore", "Restored")

    if not check_system_tools():
        print_error("–ö—Ä–∏—Ç–∏—á–Ω—ñ —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏ –≤—ñ–¥—Å—É—Ç–Ω—ñ. –ó—É–ø–∏–Ω–∫–∞.")
        sys.exit(1)

    install_brew_deps()

    # New Health Check
    check_database_health()

    if not check_venv():
        sys.exit(1)

    # Preflight: verify MCP package versions (npx invocations)
    issues = verify_mcp_package_versions()
    if issues:
        print_warning("Detected potential MCP package issues:")
        for issue in issues:
            print_warning(f"  - {issue}")
        if os.getenv("FAIL_ON_MCP_PRECHECK") == "1":
            print_error(
                "Failing setup because FAIL_ON_MCP_PRECHECK=1 and MCP precheck found issues.",
            )
            sys.exit(1)
        else:
            print_info(
                "Continuing setup despite precheck issues. Set FAIL_ON_MCP_PRECHECK=1 to fail on these errors.",
            )

    if not install_deps():
        sys.exit(1)

    # Sync configs BEFORE DB and tests to ensure latest templates are applied
    sync_configs()

    ensure_database()
    prepare_monitoring_db()
    verify_golden_fund()

    # Run detailed table verification
    asyncio.run(verify_database_tables())

    build_swift_mcp()
    build_googlemaps_mcp()

    # Google Maps Automation
    gmaps_updated = False
    try:
        gmaps_updated = setup_google_maps()
    except Exception as e:
        print_warning(f"Google Maps automation skipped: {e}")

    # Re-sync if maps were updated to ensure .env is propagated globally
    if gmaps_updated:
        print_info("–ü—Ä–æ–∫–∏–¥–∞—î–º–æ –Ω–æ–≤–∏–π API –∫–ª—é—á —É –≥–ª–æ–±–∞–ª—å–Ω—É –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—é...")
        sync_configs()

    # Provider Token Setup
    setup_provider_tokens()

    # Frontend Security & Env Verification
    ensure_frontend_config()

    setup_xcodebuild_mcp()

    # Ensure all binaries are executable
    print_step("–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ø—Ä–∞–≤ –¥–æ—Å—Ç—É–ø—É –¥–ª—è –±—ñ–Ω–∞—Ä–Ω–∏—Ö —Ñ–∞–π–ª—ñ–≤...")
    bin_dirs = [PROJECT_ROOT / "bin", PROJECT_ROOT / "vendor"]
    for bdir in bin_dirs:
        if bdir.exists():
            for root, _, files in os.walk(bdir):
                for f in files:
                    fpath = Path(root) / f
                    # If it looks like an executable (Swift binaries, MCP servers, etc)
                    if (
                        "macos-use" in f
                        or "vibe" in f
                        or "googlemaps" in f
                        or "mcp-server" in f
                        or fpath.suffix == ""
                        or "xcodebuild" in f.lower()
                    ):
                        try:
                            os.chmod(fpath, 0o755)
                        except Exception:
                            pass

    # Git Remote Auto-Configuration (github-operations workflow)
    print_step("–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è Git remote –∑ GITHUB_TOKEN...")
    try:
        remote_res = subprocess.run(
            ["git", "remote", "-v"], capture_output=True, text=True, check=False
        )
        if "github.com" in remote_res.stdout:
            print_success("Git remote –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–æ –Ω–∞ GitHub")

            # Read GITHUB_TOKEN from .env (local first, then global)
            github_token: str | None = None
            for env_path in [PROJECT_ROOT / ".env", CONFIG_ROOT / ".env"]:
                if env_path.exists():
                    with open(env_path, encoding="utf-8") as f:
                        for line in f:
                            line = line.strip()
                            if line.startswith("GITHUB_TOKEN=") and not line.startswith("#"):
                                token_val = line.split("=", 1)[1].strip().strip("'\"")
                                if token_val and not token_val.startswith("$"):
                                    github_token = token_val
                                    break
                    if github_token:
                        break

            if github_token:
                # Extract repo path from current remote URL
                remote_url = remote_res.stdout.strip().split("\n")[0]
                repo_match = re.search(r"github\.com[:/](.+?)(?:\.git)?(?:\s|$)", remote_url)
                if repo_match:
                    repo_path = repo_match.group(1).strip()
                    new_url = f"https://{github_token}@github.com/{repo_path}.git"

                    # Check if already configured with token
                    if f"@github.com/{repo_path}" in remote_res.stdout:
                        print_success("Git remote –≤–∂–µ –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–æ –∑ —Ç–æ–∫–µ–Ω–æ–º")
                    else:
                        subprocess.run(
                            ["git", "remote", "set-url", "origin", new_url],
                            check=True,
                            capture_output=True,
                        )
                        print_success(
                            "Git remote –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–æ –∑ GITHUB_TOKEN (push/pull –±–µ–∑ –ø–∞—Ä–æ–ª—é)"
                        )
                else:
                    print_warning("–ù–µ –≤–¥–∞–ª–æ—Å—è —Ä–æ–∑–ø—ñ–∑–Ω–∞—Ç–∏ GitHub repo URL")
            else:
                # No token found ‚Äî check credential helper
                helper_res = subprocess.run(
                    ["git", "config", "--get", "credential.helper"],
                    capture_output=True,
                    text=True,
                    check=False,
                )
                if "osxkeychain" in helper_res.stdout:
                    print_success("Git CLI: –ù–∞–ª–∞—à—Ç–æ–≤–∞–Ω–æ —á–µ—Ä–µ–∑ HTTPS + Keychain (osxkeychain)")
                else:
                    print_info(
                        "GITHUB_TOKEN –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –≤ .env. –î–æ–¥–∞–π—Ç–µ –π–æ–≥–æ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ–≥–æ push/pull."
                    )
                    print_info("  echo 'GITHUB_TOKEN=ghp_–≤–∞—à_—Ç–æ–∫–µ–Ω' >> .env")
        else:
            print_warning("Remote origin –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∞–±–æ –Ω–µ –≤–∫–∞–∑—É—î –Ω–∞ GitHub.")
    except Exception as e:
        print_warning(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –Ω–∞–ª–∞—à—Ç—É–≤–∞—Ç–∏ Git remote: {e}")

    try:
        download_models()
    except KeyboardInterrupt:
        print_warning("\n–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª–µ–π –ø–µ—Ä–µ—Ä–≤–∞–Ω–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–µ–º.")
    except Exception as e:
        print_error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—Ä–æ–±—Ü—ñ –º–æ–¥–µ–ª–µ–π: {e}")

    check_services()
    run_integrity_check()
    verify_llm_providers()

    # Install watchdog if missing (it might be in requirements.txt but we want to be sure for the watcher)
    print_step("–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ Watchdog –¥–ª—è –∞–≤—Ç–æ-—Å–∏–Ω—Ö—Ä–æ–Ω—ñ–∑–∞—Ü—ñ—ó...")
    try:
        import watchdog

        _ = watchdog
        print_success("Watchdog –≤–∂–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
    except ImportError:
        print_info("–í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è Watchdog...")
        subprocess.run(
            [str(VENV_PATH / "bin" / "python"), "-m", "pip", "install", "watchdog"], check=False
        )

    print_info(
        f"{Colors.OKCYAN}TIP:{Colors.ENDC} –ó–∞–ø—É—Å—Ç—ñ—Ç—å 'npm run watch:config' –¥–ª—è –∞–≤—Ç–æ-—Å–∏–Ω—Ö—Ä–æ–Ω—ñ–∑–∞—Ü—ñ—ó –∫–æ–Ω—Ñ—ñ–≥—ñ–≤"
    )

    mcp_config_path = CONFIG_ROOT / "mcp" / "config.json"
    enabled_servers = []
    if mcp_config_path.exists():
        try:
            with open(mcp_config_path, encoding="utf-8") as f:
                mcp_cfg = json.load(f)
                servers = mcp_cfg.get("mcpServers", {})
                for s_name, s_info in servers.items():
                    if not s_info.get("disabled", False):
                        enabled_servers.append(s_name)
        except Exception:
            pass

    print_success("‚úÖ –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    tracker.update("MCP Tools Integration", "Integrated")
    tracker.print_report()
    print_info("–ö—Ä–æ–∫–∏ –¥–ª—è –ø–æ—á–∞—Ç–∫—É —Ä–æ–±–æ—Ç–∏:")

    mcp_info = []
    # Try to load all servers from config to be dynamic
    if mcp_config_path.exists():
        try:
            with open(mcp_config_path, encoding="utf-8") as f:
                mcp_cfg = json.load(f)
                all_servers = mcp_cfg.get("mcpServers", {})
                for s_id, s_info in all_servers.items():
                    if s_id.startswith("_"):
                        continue  # Skip comments
                    desc = s_info.get("description", "No description available")
                    # Truncate long descriptions for cleaner output
                    if len(desc) > 80:
                        desc = desc[:77] + "..."
                    mcp_info.append((s_id, desc))
        except Exception:
            pass

    if not mcp_info:
        # Fallback to hardcoded list if config is unreadable
        mcp_info = [
            ("memory", "–ì—Ä–∞—Ñ –∑–Ω–∞–Ω—å & Long-term Memory (Python)"),
            ("macos-use", "–ù–∞—Ç–∏–≤–Ω–∏–π –∫–æ–Ω—Ç—Ä–æ–ª—å macOS + –¢–µ—Ä–º—ñ–Ω–∞–ª (Swift)"),
            ("vibe", "Coding Agent & Self-Healing (Python)"),
            ("filesystem", "–§–∞–π–ª–æ–≤—ñ –æ–ø–µ—Ä–∞—Ü—ñ—ó (Node)"),
            ("sequential-thinking", "–ì–ª–∏–±–æ–∫–µ –º–∏—Å–ª–µ–Ω–Ω—è (Node)"),
            ("xcodebuild", "Xcode Build & Test Automation (Node)"),
            ("chrome-devtools", "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—è Chrome (Node)"),
            ("puppeteer", "–í–µ–±-—Å–∫—Ä–µ–π–ø—ñ–Ω–≥ —Ç–∞ –ø–æ—à—É–∫ (Node)"),
            ("github", "–û—Ñ—ñ—Ü—ñ–π–Ω–∏–π GitHub MCP (Node)"),
            ("duckduckgo-search", "–®–≤–∏–¥–∫–∏–π –ø–æ—à—É–∫ (Python)"),
            ("whisper-stt", "–õ–æ–∫–∞–ª—å–Ω–µ —Ä–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –º–æ–≤–∏ (Python)"),
            ("graph", "–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –≥—Ä–∞—Ñ—É –∑–Ω–∞–Ω—å (Python)"),
            ("context7", "–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è –±—ñ–±–ª—ñ–æ—Ç–µ–∫ —Ç–∞ API (Node)"),
            ("devtools", "–õ—ñ–Ω—Ç–µ—Ä —Ç–∞ –∞–Ω–∞–ª—ñ–∑ –∫–æ–¥—É (Python)"),
            ("react-devtools", "React Introspection & Fiber Analysis (Node)"),
            ("redis", "–û–≥–ª—è–¥–æ–≤—ñ—Å—Ç—å –∫–µ—à—É —Ç–∞ —Å–µ—Å—ñ–π (Python)"),
            ("golden-fund", "Knowledge Base & Data Persistence (Python)"),
            ("data-analysis", "Pandas Data Analysis Engine (Python)"),
            ("googlemaps", "Google Maps API –∑ Cyberpunk —Ñ—ñ–ª—å—Ç—Ä–æ–º (Swift)"),
            ("tour-guide", "Interactive Virtual Tour Control (Internal)"),
        ]

    print_info(
        f"–î–æ—Å—Ç—É–ø–Ω—ñ MCP —Å–µ—Ä–≤–µ—Ä–∏ ({len(enabled_servers) if enabled_servers else len(mcp_info)}):"
    )
    for s_id, _ in sorted(mcp_info):
        # Check if actually enabled in config
        if enabled_servers and s_id not in enabled_servers:
            pass


def setup_provider_tokens():
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ —Ç–æ–∫–µ–Ω—ñ–≤ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ñ–≤ (—Ç—ñ–ª—å–∫–∏ —è–∫—â–æ —ó—Ö –Ω–µ–º–∞—î –≤ –ª–æ–∫–∞–ª—å–Ω–æ–º—É .env)."""
    print_step("–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ç–æ–∫–µ–Ω—ñ–≤ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ñ–≤...")

    # Check if local .env exists and has tokens
    env_path = PROJECT_ROOT / ".env"

    # Skip if .env doesn't exist (fresh install scenario)
    if not env_path.exists():
        print_info("–õ–æ–∫–∞–ª—å–Ω–∏–π .env –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ - —Ç–æ–∫–µ–Ω–∏ –±—É–¥—É—Ç—å –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ñ –ø—Ä–∏ –ø–µ—Ä—à–æ–º—É –∑–∞–ø—É—Å–∫—É")
        return

    # Load existing tokens from local .env
    existing_tokens = {}
    with open(env_path, encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line.startswith("COPILOT_API_KEY="):
                existing_tokens["copilot"] = line.split("=", 1)[1].strip()
            elif line.startswith("WINDSURF_API_KEY="):
                existing_tokens["windsurf"] = line.split("=", 1)[1].strip()

    # Check if we have valid tokens
    has_copilot = existing_tokens.get("copilot") and len(existing_tokens["copilot"]) > 10
    has_windsurf = existing_tokens.get("windsurf") and existing_tokens["windsurf"].startswith(
        "sk-ws-"
    )

    if has_copilot and has_windsurf:
        print_success("–¢–æ–∫–µ–Ω–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ñ–≤ –≤–∂–µ –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–æ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–º—É .env")
        # Sync to global location
        sync_configs()
        return

    # Only proceed if we need to setup tokens
    print_info("–î–µ—è–∫—ñ —Ç–æ–∫–µ–Ω–∏ –≤—ñ–¥—Å—É—Ç–Ω—ñ, —Å–ø—Ä–æ–±—É—î–º–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è...")

    # Setup Windsurf token (only if missing or invalid)
    if not has_windsurf:
        print_info("–°–ø—Ä–æ–±–∞ –æ—Ç—Ä–∏–º–∞—Ç–∏ Windsurf —Ç–æ–∫–µ–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ...")
        try:
            result = subprocess.run(
                [sys.executable, "-m", "providers.utils.get_windsurf_token"],
                capture_output=True,
                text=True,
                cwd=PROJECT_ROOT,
            )

            if result.returncode == 0 and "sk-ws-" in result.stdout:
                print_success("Windsurf —Ç–æ–∫–µ–Ω –æ—Ç—Ä–∏–º–∞–Ω–æ —Ç–∞ —Å–∏–Ω—Ö—Ä–æ–Ω—ñ–∑–æ–≤–∞–Ω–æ")
            else:
                print_warning("–ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ Windsurf —Ç–æ–∫–µ–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ")
                print_info("–ë—É–¥—å –ª–∞—Å–∫–∞, –∑–∞–ø—É—Å—Ç—ñ—Ç—å –≤—Ä—É—á–Ω—É:")
                print_info("  python -m providers token windsurf")
        except Exception as e:
            print_warning(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—Ç—Ä–∏–º–∞–Ω–Ω—ñ Windsurf —Ç–æ–∫–µ–Ω–∞: {e}")
    else:
        print_success("Windsurf —Ç–æ–∫–µ–Ω –≤–∂–µ –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–æ")

    # Setup Copilot token (only if missing or invalid)
    if not has_copilot:
        print_info("–î–ª—è Copilot —Ç–æ–∫–µ–Ω–∞ –ø–æ—Ç—Ä—ñ–±–Ω–∞ —Ä—É—á–Ω–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∞")
        print_info("–ë—É–¥—å –ª–∞—Å–∫–∞, –∑–∞–ø—É—Å—Ç—ñ—Ç—å:")
        print_info("  python -m providers token copilot --method vscode")
        print_info("  –ê–±–æ –æ—Ç—Ä–∏–º–∞–π—Ç–µ —Ç–æ–∫–µ–Ω –≤—Ä—É—á–Ω—É –∑ GitHub Copilot")
    else:
        print_success("Copilot —Ç–æ–∫–µ–Ω –≤–∂–µ –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–æ")

    # Sync to global location if we made changes
    # MCP Health Summary
    print()
    try:
        from src.maintenance.mcp_health import check_mcp

        asyncio.run(check_mcp())
    except Exception as e:
        print(f"‚ö†Ô∏è  –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–ø—É—Å—Ç–∏—Ç–∏ –ø–µ—Ä–µ–≤—ñ—Ä–∫—É –∑–¥–æ—Ä–æ–≤'—è MCP: {e}")

    print_info("–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ç–æ–∫–µ–Ω—ñ–≤ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ñ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")


if __name__ == "__main__":
    main()
