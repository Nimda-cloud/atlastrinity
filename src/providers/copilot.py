from __future__ import annotations

import base64
import json
import os
from collections.abc import Callable
from io import BytesIO
from typing import Any, cast

# Load environment variables from global .env
try:
    from dotenv import load_dotenv

    load_dotenv(os.path.expanduser("~/.config/atlastrinity/.env"), override=True)
except ImportError:
    pass  # dotenv not available, use system env vars

import httpx
import requests
from langchain_core.callbacks import AsyncCallbackManagerForLLMRun, CallbackManagerForLLMRun
from langchain_core.language_models import BaseChatModel
from langchain_core.messages import (
    AIMessage,
    BaseMessage,
    HumanMessage,
    SystemMessage,
)
from langchain_core.outputs import ChatGeneration, ChatResult
from PIL import Image
from tenacity import (
    retry,
    retry_if_exception,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)

from src.brain.monitoring.logger import logger

# Type aliases for better type safety
ContentItem = str | dict[str, Any]


class CopilotLLM(BaseChatModel):
    # Model translation: custom names -> real API model names
    # Note: 'oswe-vscode-secondary' IS the native ID for 'Raptor Mini' in GitHub API.
    # No custom model translation - native models only

    model_name: str | None = None
    vision_model_name: str | None = None
    api_key: str | None = None
    max_tokens: int = 4096  # Default, can be overridden per instance
    _tools: list[Any] | None = None

    def __init__(
        self,
        model_name: str | None = None,
        vision_model_name: str | None = None,
        api_key: str | None = None,
        max_tokens: int | None = None,
        **kwargs: Any,
    ) -> None:
        super().__init__(**kwargs)
        # STRICT CONFIGURATION: No hardcoded defaults
        # Import config here to avoid circular dependencies if possible, or assume it's available
        from src.brain.config.config_loader import config

        self.model_name = model_name or os.getenv("COPILOT_MODEL") or config.get("models.default")
        if not self.model_name:
            # Absolute fallback if config is broken
            self.model_name = "gpt-4o"

        vm = vision_model_name or os.getenv("COPILOT_VISION_MODEL")
        self.vision_model_name = (
            vm or config.get("models.vision") or self.model_name
        )  # Fallback to main model if vision not distinct

        # Set max_tokens (default 4096 for backward compatibility)
        self.max_tokens = max_tokens or 4096

        # Use COPILOT_API_KEY for regular models, VISION_API_KEY for vision models
        # IMPORTANT: GITHUB_TOKEN is ONLY for GitHub MCP server, NOT for agents!
        copilot_key = os.getenv("COPILOT_API_KEY")
        vision_key = os.getenv("VISION_API_KEY")

        # Determine which key to use based on model name and vision_model_name
        uses_vision_key = (
            vision_model_name is not None  # Explicit vision model
        )

        if api_key:
            self.api_key = api_key
        elif uses_vision_key and vision_key:
            self.api_key = vision_key
        elif copilot_key:
            self.api_key = copilot_key
        else:
            self.api_key = None

        if not self.api_key:
            raise RuntimeError(
                "COPILOT_API_KEY environment variable must be set. "
                "Note: GITHUB_TOKEN is only for GitHub MCP server, not for agents.",
            )

    def _has_image(self, messages: list[BaseMessage]) -> bool:
        for m in messages:
            c = getattr(m, "content", None)
            if isinstance(c, list):
                for item in c:
                    if isinstance(item, dict) and item.get("type") == "image_url":
                        return True
        return False

    @property
    def _llm_type(self) -> str:
        return "copilot-chat"

    def bind_tools(self, tools: Any, **kwargs: Any) -> CopilotLLM:
        # Store tools to describe them in the system prompt and instruct the model
        # to generate JSON tool_calls structure. MacSystemAgent calls CopilotLLM without tools,
        # so its own JSON protocol is not affected.
        if isinstance(tools, list):
            self._tools = tools
        else:
            self._tools = [tools]
        return self

    def _invoke_gemini_fallback(self, messages: list[BaseMessage]) -> AIMessage:
        try:
            # Dynamic import to avoid circular dependency

            from langchain_google_genai import (  # type: ignore[reportMissingImports]
                ChatGoogleGenerativeAI,  # pyrefly: ignore[missing-import]
            )

            api_key = os.environ.get("GEMINI_API_KEY") or os.environ.get("GEMINI_LIVE_API_KEY")
            if not api_key:
                return AIMessage(
                    content="[FALLBACK FAILED] No GEMINI_API_KEY found for vision fallback.",
                )

            llm = ChatGoogleGenerativeAI(
                model="gemini-1.5-flash",
                google_api_key=api_key,
                temperature=0.1,
            )
            return cast("AIMessage", llm.invoke(messages))
        except Exception as e:
            # If Gemini fails, try local BLIP captioning
            return self._invoke_local_blip_fallback(list(messages), e)

    def _invoke_local_blip_fallback(
        self,
        messages: list[BaseMessage],
        prior_error: Exception,
    ) -> AIMessage:
        """Ultimate fallback: Use Vision Module (OCR + BLIP) to describe the image."""
        try:
            import tempfile

            from vision_module import (  # type: ignore[reportMissingImports]
                get_vision_module,  # pyrefly: ignore[missing-import]
            )

            # Find the image in messages
            image_b64 = None
            text_parts = []
            for m in messages:
                if hasattr(m, "content") and isinstance(m.content, list):
                    for item in m.content:
                        if isinstance(item, dict):
                            if item.get("type") == "image_url":
                                url = item.get("image_url", {}).get("url", "")
                                if url.startswith("data:image"):
                                    image_b64 = url.split(",", 1)[-1]
                            elif item.get("type") == "text":
                                text_parts.append(item.get("text", ""))
                elif hasattr(m, "content") and isinstance(m.content, str):
                    text_parts.append(m.content)

            if not image_b64:
                return AIMessage(
                    content=f"[LOCAL VISION FAILED] No image found. Original error: {prior_error}",
                )

            # Decode and save to temp file

            image_bytes = base64.b64decode(image_b64)
            with tempfile.NamedTemporaryFile(suffix=".jpg", delete=False) as f:
                temp_path = f.name
                f.write(image_bytes)

            try:
                # Use Vision Module for comprehensive analysis
                vm = get_vision_module()
                analysis = vm.analyze_screenshot(temp_path, mode="auto")

                # Build description
                descriptions = []

                if analysis.get("combined_description"):
                    descriptions.append(analysis["combined_description"])

                # Check for numbers specifically (for calculator-like scenarios)
                ocr_result = analysis.get("analyses", {}).get("ocr", {})
                if ocr_result.get("status") == "success":
                    text = ocr_result.get("text", "")
                    if text:
                        # Extract numbers
                        import re

                        numbers = re.findall(r"-?[\d,]+\.?\d*", text)
                        if numbers:
                            descriptions.append(f"Numbers detected: {', '.join(numbers[:5])}")

                combined_desc = (
                    "\n".join(descriptions) if descriptions else "Could not analyze image."
                )

                # Reconstruct message for LLM
                original_text = "\n".join(text_parts) if text_parts else "Analyze the screenshot."
                new_prompt = f"{original_text}\n\n[AUTOMATIC IMAGE ANALYSIS (OCR + BLIP)]:\n{combined_desc}\n\nBased on this analysis, what can you say about the screen state? Respond strictly in JSON format."

                # Call LLM with text-only message

                text_only_messages: list[BaseMessage] = cast(
                    "list[BaseMessage]",
                    [msg for msg in messages if isinstance(msg, SystemMessage)]
                    + [
                        HumanMessage(content=new_prompt),
                    ],
                )

                return self._internal_text_invoke(text_only_messages)

            finally:
                os.unlink(temp_path)

        except Exception as e:
            return AIMessage(content=f"[LOCAL VISION FAILED] {e}. Prior error: {prior_error}")

    def _internal_text_invoke(self, messages: list[BaseMessage]) -> AIMessage:
        """Internal text-only invoke for fallback scenarios (no image processing)"""
        try:
            result = self._generate(messages)
            if result.generations:
                return cast("AIMessage", result.generations[0].message)
            return AIMessage(content="[No response generated]")
        except Exception as e:
            return AIMessage(content=f"[Internal invoke error] {e}")

    @retry(
        stop=stop_after_attempt(2),
        wait=wait_exponential(multiplier=1, min=1, max=3),
        retry=retry_if_exception_type(
            (requests.Timeout, requests.ConnectionError, requests.HTTPError)
        ),
    )
    def _get_session_token(self) -> tuple[str, str]:
        headers = {
            "Authorization": f"token {self.api_key}",
            "Editor-Version": "vscode/1.85.0",
            "Editor-Plugin-Version": "copilot/1.144.0",
            "User-Agent": "GithubCopilot/1.144.0",
        }
        try:
            response = requests.get(
                "https://api.github.com/copilot_internal/v2/token",
                headers=headers,
                timeout=15,
            )
            response.raise_for_status()
            data = response.json()
            token = data.get("token")
            api_endpoint = data.get("endpoints", {}).get("api") or "https://api.githubcopilot.com"
            if not token:
                raise RuntimeError("Copilot token response missing 'token' field.")
            return token, api_endpoint
        except requests.HTTPError:
            # During tests we may set COPILOT_API_KEY to a dummy value; in that case
            # return a dummy token instead of raising an error to avoid network calls.
            if str(self.api_key).lower() in {"dummy", "test"} or os.getenv(
                "COPILOT_API_KEY",
                "",
            ).lower() in {"dummy", "test"}:
                return "dummy-session-token", "https://api.githubcopilot.com"
            raise
        except Exception:
            # Other errors: propagate
            raise

    def _build_payload(self, messages: list[BaseMessage], stream: bool | None = None) -> dict:
        formatted_messages = []

        # Extract system prompt if present, or use default
        system_content = "You are a helpful AI assistant."

        # Tool instructions - English now
        if self._tools:
            tools_desc_lines: list[str] = []
            for tool in self._tools:
                if isinstance(tool, dict):
                    name = tool.get("name", "tool")
                    description = tool.get("description", "")
                else:
                    name = getattr(tool, "name", getattr(tool, "__name__", "tool"))
                    description = getattr(tool, "description", "")
                tools_desc_lines.append(f"- {name}: {description}")
            tools_desc = "\n".join(tools_desc_lines)

            tool_instructions = (
                "CRITICAL: If you need to use tools, you MUST respond ONLY in the following JSON format. "
                "Any other text outside the JSON will cause a system error.\n\n"
                "AVAILABLE TOOLS:\n"
                f"{tools_desc}\n\n"
                "JSON FORMAT (ONLY IF USING TOOLS):\n"
                "{\n"
                '  "tool_calls": [\n'
                '    { "name": "tool_name", "args": { ... } }\n'
                "  ],\n"
                '  "final_answer": "Immediate feedback in UKRAINIAN (e.g., \'Зараз перевірю...\')."\n'
                "}\n\n"
                "If text response is enough (no tools needed), answer normally in Ukrainian.\n"
                "If you ALREADY checked results (ToolMessages provided), provide a final summary in plain text.\n"
            )
        else:
            tool_instructions = ""

        for m in messages:
            role = "user"
            if isinstance(m, SystemMessage):
                role = "system"
                if isinstance(m.content, str):
                    system_content = m.content + (
                        "\n\n" + tool_instructions if tool_instructions else ""
                    )
                else:
                    system_content = str(m.content) + (
                        "\n\n" + tool_instructions if tool_instructions else ""
                    )
                continue
            if isinstance(m, AIMessage):
                role = "assistant"
            elif isinstance(m, HumanMessage):
                role = "user"

            # Handle list content (Vision)
            content = m.content
            if isinstance(content, list):
                processed_content: list[ContentItem] = []
                for item in content:
                    if isinstance(item, dict) and item.get("type") == "image_url":
                        # Optimize image if needed
                        url = item.get("image_url", {}).get("url", "")
                        if url.startswith("data:image"):
                            try:
                                optimized_url = self._optimize_image_b64(url)
                                processed_content.append(
                                    {"type": "image_url", "image_url": {"url": optimized_url}},
                                )
                            except Exception:
                                processed_content.append(item)
                        else:
                            processed_content.append(item)
                    else:
                        processed_content.append(item)
                content = processed_content

            formatted_messages.append({"role": role, "content": content})

        # Prepend system message
        final_messages = [{"role": "system", "content": system_content}, *formatted_messages]

        # Choose model based on whether we have images
        chosen_model = self.vision_model_name if self._has_image(messages) else self.model_name

        return {
            "model": chosen_model,
            "messages": final_messages,
            "temperature": 0.1,
            "max_tokens": self.max_tokens,
            "stream": stream if stream is not None else False,
        }

    def _optimize_image_b64(self, data_url: str) -> str:
        """Resize and compress image for stability"""
        try:
            _header, encoded = data_url.split(",", 1)
            image_bytes = base64.b64decode(encoded)
            img: Image.Image = Image.open(BytesIO(image_bytes))

            # Max dimension 1280 (OpenAI high res limit without extra tiles)
            max_size = 1280
            if max(img.size) > max_size:
                ratio = max_size / max(img.size)
                new_size = (int(img.size[0] * ratio), int(img.size[1] * ratio))
                # modern PIL uses Resampling.LANCZOS
                try:
                    resampling = Image.Resampling.LANCZOS  # type: ignore
                except AttributeError:
                    resampling = Image.LANCZOS  # type: ignore
                img = img.resize(new_size, resampling)

            if img.mode in ("RGBA", "P"):
                img = img.convert("RGB")

            buffered = BytesIO()
            img.save(buffered, format="JPEG", quality=80)
            return f"data:image/jpeg;base64,{base64.b64encode(buffered.getvalue()).decode()}"
        except Exception:
            return data_url

    async def _agenerate(
        self,
        messages: list[BaseMessage],
        stop: list[str] | None = None,
        run_manager: AsyncCallbackManagerForLLMRun | None = None,
        **kwargs: Any,
    ) -> ChatResult:
        """Asynchronous generation using httpx with automatic model fallback on 400 errors"""
        import tenacity

        def _is_transient_error(exception: BaseException) -> bool:
            is_transient = False
            if isinstance(exception, httpx.HTTPStatusError):
                is_transient = exception.response.status_code in [429, 500, 502, 503, 504]
            else:
                is_transient = isinstance(
                    exception,
                    (
                        httpx.ConnectError,
                        httpx.TimeoutException,
                        httpx.NetworkError,
                        httpx.RemoteProtocolError,
                    ),
                )
            if is_transient:
                error_body = getattr(exception.response, 'text', 'No body') if isinstance(exception, httpx.HTTPStatusError) else str(exception)
                logger.warning(f"[COPILOT] Transient error detected: {exception}. Body: {error_body[:500]}. Retrying...")
            return is_transient

        def _log_retry_attempt(retry_state):
            if retry_state.attempt_number > 1:
                logger.info(f"[COPILOT] Retry attempt {retry_state.attempt_number} for _agenerate")

        # Use tenacity for retrying on network errors
        @tenacity.retry(
            stop=tenacity.stop_after_attempt(5),
            wait=tenacity.wait_exponential(multiplier=1, min=2, max=10),
            retry=tenacity.retry_if_exception(_is_transient_error),
            before_sleep=_log_retry_attempt,
            reraise=True,
        )
        async def _do_post(client, url, headers, json):
            response = await client.post(url, headers=headers, json=json)
            # Raise exception for 429 and 5xx to trigger tenacity retry
            if response.status_code in [429, 500, 502, 503, 504]:
                logger.debug(f"[COPILOT] Received status {response.status_code}, raising to retry")
                response.raise_for_status()
            return response

        try:
            session_token, api_endpoint = self._get_session_token()

            headers = {
                "Authorization": f"Bearer {session_token}",
                "Content-Type": "application/json",
                "Editor-Version": "vscode/1.85.0",
                "Copilot-Vision-Request": "true" if self._has_image(messages) else "false",
            }
            payload = self._build_payload(messages)

            async with httpx.AsyncClient(timeout=httpx.Timeout(300.0, connect=30.0)) as client:
                response = await _do_post(
                    client,
                    f"{api_endpoint}/chat/completions",
                    headers,
                    payload,
                )

                if response.status_code == 400:
                    # Parse error to determine type
                    try:
                        error_json = response.json()
                        error_code = error_json.get("error", {}).get("code", "")
                        if error_code == "model_not_supported":
                            pass
                    except:
                        pass

                    # Use a fallback model from environment or default to a config value
                    fallback_model = os.getenv("COPILOT_FALLBACK_MODEL", "default_fallback_model")
                    payload["model"] = fallback_model

                    # Clean headers and payload for fallback
                    headers_fb = headers.copy()
                    # Remove vision-related headers
                    headers_fb.pop("Copilot-Vision-Request", None)
                    headers_fb.pop("X-Request-Id", None)

                    payload_fb = payload.copy()
                    if "messages" in payload_fb:
                        cleaned_messages = []
                        for msg in payload_fb["messages"]:
                            content = msg.get("content")
                            if isinstance(content, list):
                                # Extract only text content, remove images
                                text_parts = []
                                for item in content:
                                    if isinstance(item, dict):
                                        if item.get("type") == "text":
                                            text_parts.append(item.get("text", ""))
                                        elif item.get("type") == "image_url":
                                            text_parts.append(
                                                "[Image content removed for compatibility]"
                                            )
                                text_only = " ".join(text_parts)
                                cleaned_messages.append(
                                    {
                                        **msg,
                                        "content": text_only or "[Content processed for fallback]",
                                    },
                                )
                            else:
                                cleaned_messages.append(msg)
                        payload_fb["messages"] = cleaned_messages

                    # Reduce temperature for more reliable fallback
                    payload_fb["temperature"] = min(payload_fb.get("temperature", 0.7), 0.5)

                    retry_response = await _do_post(
                        client,
                        f"{api_endpoint}/chat/completions",
                        headers_fb,
                        payload_fb,
                    )

                    if retry_response.status_code != 200:
                        pass
                    retry_response.raise_for_status()

                    return self._process_json_result(retry_response.json(), messages)

                response.raise_for_status()
                data = response.json()

            return self._process_json_result(data, messages)
        except Exception as e:
            return ChatResult(
                generations=[ChatGeneration(message=AIMessage(content=f"[COPILOT ERROR] {e}"))],
            )

    def _process_json_result(self, data: dict[str, Any], messages: list[BaseMessage]) -> ChatResult:
        """Shared logic to process model response"""
        if not data.get("choices"):
            return ChatResult(
                generations=[
                    ChatGeneration(message=AIMessage(content="[COPILOT] No response choice.")),
                ],
            )

        content = data["choices"][0]["message"]["content"]

        if not self._tools:
            return ChatResult(generations=[ChatGeneration(message=AIMessage(content=content))])

        tool_calls = []
        final_answer = ""
        try:
            json_start = content.find("{")
            json_end = content.rfind("}")
            if json_start >= 0 and json_end >= 0:
                parse_candidate = content[json_start : json_end + 1]
                parsed = json.loads(parse_candidate)
            else:
                parsed = json.loads(content)

            if isinstance(parsed, dict):
                calls = parsed.get("tool_calls") or []
                for idx, call in enumerate(calls):
                    tool_calls.append(
                        {
                            "id": f"call_{idx}",
                            "type": "tool_call",
                            "name": call.get("name"),
                            "args": call.get("args") or {},
                        },
                    )
                final_answer = str(parsed.get("final_answer", ""))
        except Exception:
            return ChatResult(generations=[ChatGeneration(message=AIMessage(content=content))])

        if tool_calls:
            return ChatResult(
                generations=[
                    ChatGeneration(message=AIMessage(content=final_answer, tool_calls=tool_calls)),
                ],
            )
        return ChatResult(
            generations=[ChatGeneration(message=AIMessage(content=final_answer or content))],
        )

    def _generate(
        self,
        messages: list[BaseMessage],
        stop: list[str] | None = None,
        run_manager: CallbackManagerForLLMRun | None = None,
        **kwargs: Any,
    ) -> ChatResult:
        """Synchronous generation with proper error handling"""
        session_token = None
        api_endpoint = "https://api.githubcopilot.com"
        headers = {}
        payload = {}

        try:
            session_token, api_endpoint = self._get_session_token()
            headers = {
                "Authorization": f"Bearer {session_token}",
                "Content-Type": "application/json",
                "Editor-Version": "vscode/1.85.0",
                "Copilot-Vision-Request": "true" if self._has_image(messages) else "false",
            }
            payload = self._build_payload(messages)

            def _is_transient_requests_error(exception: BaseException) -> bool:
                if isinstance(exception, requests.HTTPError):
                    error_body = exception.response.text if exception.response else "No body"
                    is_trans = exception.response is not None and exception.response.status_code in [429, 500, 502, 503, 504]
                    if is_trans:
                        logger.warning(f"[COPILOT] Sync transient error: {exception}. Body: {error_body[:500]}")
                    return is_trans
                return isinstance(exception, (requests.Timeout, requests.ConnectionError))

            @retry(
                stop=stop_after_attempt(5),
                wait=wait_exponential(multiplier=1, min=2, max=10),
                retry=retry_if_exception(_is_transient_requests_error),
                reraise=True,
            )
            def _do_sync_post():
                response = requests.post(
                    f"{api_endpoint}/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=300,
                )
                if response.status_code in [429, 500, 502, 503, 504]:
                    response.raise_for_status()
                return response

            response = _do_sync_post()
            response.raise_for_status()
            return self._process_json_result(response.json(), messages)

        except requests.exceptions.HTTPError as e:
            return self._handle_vision_fallback(e, headers, payload, messages, api_endpoint)
        except Exception as e:
            return ChatResult(
                generations=[ChatGeneration(message=AIMessage(content=f"[COPILOT ERROR] {e}"))],
            )

    def _handle_vision_fallback(
        self,
        e: requests.exceptions.HTTPError,
        headers: dict[str, str],
        payload: dict[str, Any],
        messages: list[BaseMessage],
        api_endpoint: str = "https://api.githubcopilot.com",
    ) -> ChatResult:
        # Check for Vision error (400) and try fallback
        if not (
            hasattr(e, "response") and e.response is not None and e.response.status_code == 400
        ):
            status = e.response.status_code if hasattr(e, "response") and e.response else "Unknown"
            error_msg = f"[COPILOT ERROR] HTTP {status}: {e}"
            return ChatResult(generations=[ChatGeneration(message=AIMessage(content=error_msg))])

        try:
            # Clean headers and messages for fallback
            headers.pop("Copilot-Vision-Request", None)
            headers.pop("X-Request-Id", None)

            if "messages" in payload:
                # Clean messages for fallback - remove image content
                cleaned_messages = []
                for msg in payload["messages"]:
                    content = msg.get("content")
                    if isinstance(content, list):
                        # Extract only text content, remove images
                        text_parts = []
                        for item in content:
                            if isinstance(item, dict):
                                if item.get("type") == "text":
                                    text_parts.append(item.get("text", ""))
                                elif item.get("type") == "image_url":
                                    text_parts.append("[Image content removed for compatibility]")
                        text_only = " ".join(text_parts)
                        cleaned_messages.append(
                            {
                                **msg,
                                "content": text_only or "[Content processed for fallback]",
                            },
                        )
                    else:
                        cleaned_messages.append(msg)
                payload["messages"] = cleaned_messages

            payload["model"] = os.getenv("COPILOT_FALLBACK_MODEL", "gpt-4o")
            payload["temperature"] = min(payload.get("temperature", 0.7), 0.5)

            @retry(
                stop=stop_after_attempt(2),
                wait=wait_exponential(multiplier=1, min=2, max=5),
                retry=retry_if_exception_type(
                    (requests.exceptions.Timeout, requests.exceptions.ConnectionError),
                ),
                reraise=True,
            )
            def _post_retry():
                return requests.post(
                    f"{api_endpoint}/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=300,
                )

            retry_response = _post_retry()
            retry_response.raise_for_status()
            data = retry_response.json()

            if not data.get("choices"):
                return ChatResult(
                    generations=[
                        ChatGeneration(message=AIMessage(content="[COPILOT] No response."))
                    ],
                )

            content = data["choices"][0]["message"]["content"]
            return ChatResult(generations=[ChatGeneration(message=AIMessage(content=content))])

        except Exception as retry_err:
            return ChatResult(
                generations=[
                    ChatGeneration(
                        message=AIMessage(content=f"[COPILOT ERROR] Fallback failed: {retry_err}")
                    )
                ],
            )

    def _stream_response(
        self,
        response: requests.Response,
        messages: list[BaseMessage],
        on_delta: Callable[[str], None] | None = None,
    ) -> ChatResult:
        """Handle streaming response from Copilot API."""
        content = ""
        for line in response.iter_lines():
            if line:
                content = self._parse_copilot_stream_line(line, content, on_delta)

        ai_msg = self._build_final_ai_message(content)
        return ChatResult(generations=[ChatGeneration(message=ai_msg)])

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10),
        retry=retry_if_exception_type(
            (requests.exceptions.Timeout, requests.exceptions.ConnectionError),
        ),
        reraise=True,
    )
    def invoke_with_stream(
        self,
        messages: list[BaseMessage],
        *,
        on_delta: Callable[[str], None] | None = None,
    ) -> AIMessage:
        session_token, api_endpoint = self._get_session_token()

        # Only add Vision header when there are actual images in the messages
        has_images = self._has_image(messages)
        headers = {
            "Authorization": f"Bearer {session_token}",
            "Content-Type": "application/json",
            "Editor-Version": "vscode/1.85.0",
        }
        if has_images:
            headers["Copilot-Vision-Request"] = "true"

        payload = self._build_payload(messages, stream=True)

        @retry(
            stop=stop_after_attempt(3),
            wait=wait_exponential(multiplier=1, min=4, max=10),
            retry=retry_if_exception_type(
                (requests.exceptions.Timeout, requests.exceptions.ConnectionError),
            ),
            reraise=True,
        )
        def _post_stream():
            return requests.post(
                f"{api_endpoint}/chat/completions",
                headers=headers,
                data=json.dumps(payload),
                stream=True,
                timeout=300,
            )

        try:
            response = _post_stream()
        except Exception as e:
            return AIMessage(content=f"[COPILOT ERROR] Failed to connect: {e}")
        # If we are in a test mode (dummy token), skip network call and synthesize response
        if (
            str(session_token).startswith("dummy")
            or str(self.api_key).lower() in {"dummy", "test"}
            or os.getenv("COPILOT_API_KEY", "").lower() in {"dummy", "test"}
        ):
            # Return the last human message content as the AI response for tests
            content = ""
            try:
                for m in reversed(messages):
                    if isinstance(m, HumanMessage):
                        content = getattr(m, "content", "") or ""
                        break
            except Exception:
                content = "[TEST DUMMY RESPONSE]"
            return AIMessage(content=content)
        response.raise_for_status()

        content = ""
        for line in response.iter_lines():
            if not line:
                continue
            content = self._parse_copilot_stream_line(line, content, on_delta)
        return self._build_final_ai_message(content)

    def _parse_copilot_stream_line(self, line: bytes, content: str, on_delta: Any) -> str:
        decoded = line.decode("utf-8")
        if not decoded.startswith("data: "):
            return content
        data_str = decoded[6:]
        if data_str.strip() == "[DONE]":
            return content
        try:
            data = json.loads(data_str)
            delta = data["choices"][0].get("delta", {})
            piece = delta.get("content")
            if piece:
                content += piece
                if on_delta:
                    on_delta(piece)
        except (json.JSONDecodeError, KeyError, IndexError):
            pass
        return content

    def _build_final_ai_message(self, content: str) -> AIMessage:
        tool_calls = []
        if self._tools and content:
            try:
                json_start = content.find("{")
                json_end = content.rfind("}")
                if json_start >= 0 and json_end >= 0:
                    parse_candidate = content[json_start : json_end + 1]
                    parsed = json.loads(parse_candidate)
                    if isinstance(parsed, dict):
                        calls = parsed.get("tool_calls") or []
                        if isinstance(calls, list):
                            for idx, call in enumerate(calls):
                                name = call.get("name")
                                if not name:
                                    continue
                                args = call.get("args") or {}
                                tool_calls.append(
                                    {
                                        "id": f"call_{idx}",
                                        "type": "tool_call",
                                        "name": name,
                                        "args": args,
                                    },
                                )
                        final_answer = str(parsed.get("final_answer", ""))
                        if tool_calls:
                            content = final_answer or ""
                        elif final_answer:
                            content = final_answer
            except Exception:
                pass
        return AIMessage(content=content, tool_calls=tool_calls)
